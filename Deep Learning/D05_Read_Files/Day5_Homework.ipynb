{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day5_Homework.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM2VKkkf32Imhtmv2G8oNRs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tahh3KYWRuYy","executionInfo":{"status":"ok","timestamp":1616658449720,"user_tz":-480,"elapsed":889,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}},"outputId":"bddae375-dd2d-42f0-e547-1cdbe40bfed4"},"source":["# Import torch and other required modules\n","import glob\n","import torch\n","import re\n","import os\n","import nltk\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","from sklearn.datasets import load_svmlight_file\n","from nltk.corpus import stopwords\n","from torch.nn.utils.rnn import pad_sequence\n","nltk.download('stopwords') \n","nltk.download('punkt')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6m3vZRgR4Uc","executionInfo":{"status":"ok","timestamp":1616657313918,"user_tz":-480,"elapsed":12004,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}},"outputId":"6fba3104-373a-40be-e7e0-b5a271f2430e"},"source":["!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -zxf aclImdb_v1.tar.gz"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2021-03-25 07:28:22--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n","Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 84125825 (80M) [application/x-gzip]\n","Saving to: ‘aclImdb_v1.tar.gz’\n","\n","aclImdb_v1.tar.gz   100%[===================>]  80.23M  23.9MB/s    in 4.5s    \n","\n","2021-03-25 07:28:27 (17.7 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSXI2p7jSMFy","executionInfo":{"status":"ok","timestamp":1616657510548,"user_tz":-480,"elapsed":624,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}},"outputId":"f11174e7-8a7c-45ef-876a-429584bd1ac2"},"source":["with open (\"/content/aclImdb/imdb.vocab\",encoding = \"utf-8\") as f:\n","  vocab = [line.strip() for line in f.readlines()]\n","print(f\"vocab length before removing stopwords: {len(vocab)}\")\n","en_stopwords = set(stopwords.words('english'))\n","vocab = [word for word in vocab if word not in en_stopwords]\n","print(f\"vocab length before removing stopwords: {len(vocab)}\")\n","vocab_dic = {word: idx for idx, word in enumerate(vocab)}"],"execution_count":6,"outputs":[{"output_type":"stream","text":["vocab length before removing stopwords: 89527\n","vocab length before removing stopwords: 89356\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vloaJa7yS6FO","executionInfo":{"status":"ok","timestamp":1616657702818,"user_tz":-480,"elapsed":764,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}},"outputId":"b0338c97-7199-4f80-8bce-516ae417ff11"},"source":["review_pairs = []\n","for folder, label in [('pos',1),('neg',0)]:\n","  filepaths = glob.glob(os.path.join('aclImdb', 'train', folder,'*'))\n","  for filepath in filepaths:\n","    review_pairs.append((filepath,label))\n","\n","print(review_pairs[:2])\n","print(f\"Total reviews:{len(review_pairs)}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[('aclImdb/train/pos/6217_8.txt', 1), ('aclImdb/train/pos/6231_10.txt', 1)]\n","Total reviews:25000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"abGyUPJiTrva","executionInfo":{"status":"ok","timestamp":1616658710206,"user_tz":-480,"elapsed":629,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}}},"source":["def load_review(review_path):\n","    with open(review_path, encoding='utf-8') as f:\n","        review = f.read()\n","\n","    # 移除non-alphabet符號、贅字與tokenize\n","    review = re.sub(r'\\W', ' ', review)\n","    review = nltk.word_tokenize(review)\n","    \n","    return review\n","\n","def generate_vec(review, vocab_dic):\n","    idx_vec = [vocab_dic[word] for word in review if vocab_dic.get(word)]\n","\n","    return idx_vec"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"5w58QtskVA3s","executionInfo":{"status":"ok","timestamp":1616658724874,"user_tz":-480,"elapsed":883,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}}},"source":["class dataset(Dataset):\n","    '''custom dataset to load reviews and labels\n","    Parameters\n","    ----------\n","    data_pairs: list\n","        directory of all review-label pairs\n","    vocab: list\n","        list of vocabularies\n","    '''\n","    def __init__(self, data_dirs, vocab):\n","        self.data_dirs = data_dirs\n","        self.vocab = vocab\n","\n","    def __len__(self):\n","        return len(self.data_dirs)\n","\n","    def __getitem__(self, idx):\n","        review_path, label = self.data_dirs[idx]\n","        review = load_review(review_path)\n","        idx_vector = generate_vec(review, self.vocab)\n","\n","        return idx_vector, label\n","    \n","\n","# 建立客製化collate_fn，將長度不一的文本pad 0變成相同長度\n","def collate_fn(batch):\n","    reviews, labels = zip(*batch)\n","    lengths = torch.LongTensor([len(review) for review in reviews])\n","    labels = torch.LongTensor(labels)\n","    reviews = pad_sequence([\n","        torch.LongTensor(review) for review in reviews\n","    ], batch_first=True, padding_value=0)\n","\n","    return reviews, labels, lengths"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZUZ2pF2WOYM","executionInfo":{"status":"ok","timestamp":1616658725996,"user_tz":-480,"elapsed":737,"user":{"displayName":"Vincent Chen","photoUrl":"","userId":"17395139461018792477"}},"outputId":"fa8d726c-5cfc-4d95-fccb-af113453148b"},"source":["custom_dataset = dataset(review_pairs, vocab_dic)\n","custom_dataloader = DataLoader(custom_dataset, \n","                               batch_size=4, \n","                               sampler=RandomSampler(custom_dataset), \n","                               collate_fn=collate_fn\n",")\n","next(iter(custom_dataloader))"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 2736,    78,   261,    78, 11250,   981,  6606,   247,  1167,  1283,\n","           3662,  3120,  1130,  2196,   338,   981,    15, 25205,    78,    11,\n","             65,    51,  1662,  1777,  7138,  4059,  1681,    78,    84,   704,\n","           1167,  3207, 19752, 17304,    78,   263,  7337,  2186,  1900,  5511,\n","          15029,    78,   100,    65,   506,   297,  2223,  6303,  4100,   625,\n","            293,    98,    75,    49,     4,    17,  5398,    65,   734,  4862,\n","            572,  6827,    98,  4411, 21732,    49,   306, 75154, 75154,    27,\n","           1033,   352,     2,   733,   590,   628,   215,  2040,   432,    49,\n","            514,  1389,  1413,   686, 75154, 75154,    18,   145,   568,   352,\n","            476,  4543,  1606,  1606,  1606],\n","         [   42,   265,  4186,  2996,  4303,  2895, 13254,  1105,    14,    15,\n","             15,  7806,   208,    71,   457,  8055,   166,    77,    42,   422,\n","             89,   242,     4,    98,   218,  1213,   852,   321,    59,   452,\n","           1181,  4095,  2264,   378,  2780,  1979,    20,  2010,  8674,   103,\n","            288, 56328,    33,   127,   232,   448,    48,  2578,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0],\n","         [ 5582,    70,     6,   867,    82,    92,   186,  3536,  5164,    70,\n","              4,  1147,   262,    34,   262,  1908,   509,   193,    26,   979,\n","           3891,   134,   353,   111,  2010,  9770,  2081,   115,   813,    61,\n","             86,   101,    18,  3022,   143,  2437,   127,  2156,   127,    80,\n","             41,    83,   295,   979,  1908,  2426,    24,   814,    91,   157,\n","             56,    78,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0],\n","         [  138,  1069,     8,   191,  1050,   826,  1845,    15,   236,    63,\n","            371,  6212,   415,    70,   331,   120,   276,  5160,  7427,  1221,\n","           7427,  4598, 12432, 11899, 75154, 75154,  1994,  6487,  1208,   161,\n","            271,   292,  1845,  4152,   362,   848,   552,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","              0,     0,     0,     0,     0]]),\n"," tensor([0, 0, 0, 0]),\n"," tensor([95, 48, 52, 37]))"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"M4-nFYSgWQml"},"source":[""],"execution_count":null,"outputs":[]}]}