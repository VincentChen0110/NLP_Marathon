{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day22_Seq2Seq_translator_en_de.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrWWUhFlUprI"
      },
      "source": [
        "# Homework : Neural Machine Translation\n",
        "***\n",
        "## [Objective]\n",
        "Pytorch De-En Translator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "Lzjl5xnFTm6l",
        "outputId": "085edc9d-d4d2-489a-f227-3702c9e989ac"
      },
      "source": [
        "!pip uninstall spacy -y\n",
        "!pip install -U spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: spacy 3.1.1\n",
            "Uninstalling spacy-3.1.1:\n",
            "  Successfully uninstalled spacy-3.1.1\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: spacy\n",
            "Successfully installed spacy-3.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "6Kd-FoO8WYbT",
        "outputId": "3345b146-0387-40cf-fcda-e3f883744e25"
      },
      "source": [
        "!pip install torchtext==0.8.1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.1\n",
            "  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.41.1)\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1 torchtext-0.8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RanKHsWTu-rn"
      },
      "source": [
        "import jieba\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import random\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pprint import pprint\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyskUGjGSr-0",
        "outputId": "08a63446-9140-48ad-ce51-4b11726ba7e1"
      },
      "source": [
        "!mkdir ./data\n",
        "!mkdir ./data/multi30k\n",
        "!python -m spacy download en\n",
        "!ls ./data/multi30k -al\n",
        "spacy_english = spacy.load(\"en_core_web_sm\")\n",
        "!ls ./data/multi30k -al"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "mkdir: cannot create directory ‘./data/multi30k’: File exists\n",
            "2021-07-21 14:21:16.441771: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.6 MB 165 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.1.0) (3.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.2.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jul 21 13:25 .\n",
            "drwxr-xr-x 3 root root 4096 Jul 21 13:25 ..\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jul 21 13:25 .\n",
            "drwxr-xr-x 3 root root 4096 Jul 21 13:25 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hjr_6AXTRoz",
        "outputId": "978abab5-9ae8-44c6-a809-3834284d68fc"
      },
      "source": [
        "!python -m spacy download de\n",
        "spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "!ls ./data/multi30k -al"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-21 14:21:24.289498: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.1.0/de_core_news_sm-3.1.0-py3-none-any.whl (18.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.1.0) (3.1.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (8.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (57.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (21.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.4)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (5.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jul 21 13:25 .\n",
            "drwxr-xr-x 3 root root 4096 Jul 21 13:25 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK984GbYv8Y-",
        "outputId": "4832b754-f73b-4025-c260-238e7fb69321"
      },
      "source": [
        "\n",
        "def tokenize_de(text):\n",
        "  return [token.text for token in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_english(text):\n",
        "  return [token.text for token in spacy_english.tokenizer(text)]\n",
        "\n",
        "### Sample Run ###\n",
        "\n",
        "sample_text = \"I love machine learning\"\n",
        "print(tokenize_english(sample_text))\n",
        "\n",
        "german = Field(tokenize=tokenize_de, lower=True,\n",
        "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "english = Field(tokenize=tokenize_english, lower=True,\n",
        "               init_token=\"<sos>\", eos_token=\"<eos>\")\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),\n",
        "                                                    fields=(german, english))\n",
        "\n",
        "german.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "english.build_vocab(train_data, max_size=10000, min_freq=3)\n",
        "\n",
        "print(f\"Unique tokens in source (german) vocabulary: {len(german.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'love', 'machine', 'learning']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (german) vocabulary: 5374\n",
            "Unique tokens in target (en) vocabulary: 4556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ3fiLno60K_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "4e07db37-24f4-4764-aca9-38ac1fecf225"
      },
      "source": [
        "word_2_idx = dict(english.vocab)\n",
        "idx_2_word = {}\n",
        "for k,v in word_2_idx.items():\n",
        "  idx_2_word[v] = k"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9dd736a1d600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_2_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0midx_2_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_2_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0midx_2_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WunTmSIJzBaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e214c2c-c928-4266-9d6d-882d547a345f"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "print(train_data[5].__dict__.keys())\n",
        "pprint(train_data[5].__dict__.values())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n",
            "dict_keys(['src', 'trg'])\n",
            "dict_values([['ein', 'mann', 'in', 'grün', 'hält', 'eine', 'gitarre', ',', 'während', 'der', 'andere', 'mann', 'sein', 'hemd', 'ansieht', '.'], ['a', 'man', 'in', 'green', 'holds', 'a', 'guitar', 'while', 'the', 'other', 'man', 'observes', 'his', 'shirt', '.']])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRGP9EsizRRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f946b449-75bc-440b-eff6-0f9a98a91158"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
        "                                                                      batch_size = BATCH_SIZE, \n",
        "                                                                      sort_within_batch=True,\n",
        "                                                                      sort_key=lambda x: len(x.src),\n",
        "                                                                      device = device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3nozOT8zdeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb8e3d5-d82b-4688-fa5a-40f11c238908"
      },
      "source": [
        "count = 0\n",
        "max_len_eng = []\n",
        "max_len_ger = []\n",
        "for data in train_data:\n",
        "  max_len_ger.append(len(data.src))\n",
        "  max_len_eng.append(len(data.trg))\n",
        "  if count < 10 :\n",
        "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
        "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    print()\n",
        "  count += 1\n",
        "\n",
        "print(\"Maximum Length of English sentence {} and German sentence {} in the dataset\".format(max(max_len_eng),max(max_len_ger)))\n",
        "print(\"Minimum Length of English sentence {} and German sentence {} in the dataset\".format(min(max_len_eng),min(max_len_ger)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "German -  zwei junge weiße männer sind im freien in der nähe vieler büsche .  Length -  13\n",
            "English -  two young , white males are outside near many bushes .  Length -  11\n",
            "\n",
            "German -  mehrere männer mit schutzhelmen bedienen ein antriebsradsystem .  Length -  8\n",
            "English -  several men in hard hats are operating a giant pulley system .  Length -  12\n",
            "\n",
            "German -  ein kleines mädchen klettert in ein spielhaus aus holz .  Length -  10\n",
            "English -  a little girl climbing into a wooden playhouse .  Length -  9\n",
            "\n",
            "German -  ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster .  Length -  15\n",
            "English -  a man in a blue shirt is standing on a ladder cleaning a window .  Length -  15\n",
            "\n",
            "German -  zwei männer stehen am herd und bereiten essen zu .  Length -  10\n",
            "English -  two men are at the stove preparing food .  Length -  9\n",
            "\n",
            "German -  ein mann in grün hält eine gitarre , während der andere mann sein hemd ansieht .  Length -  16\n",
            "English -  a man in green holds a guitar while the other man observes his shirt .  Length -  15\n",
            "\n",
            "German -  ein mann lächelt einen ausgestopften löwen an .  Length -  8\n",
            "English -  a man is smiling at a stuffed lion  Length -  8\n",
            "\n",
            "German -  ein schickes mädchen spricht mit dem handy während sie langsam die straße entlangschwebt .  Length -  14\n",
            "English -  a trendy girl talking on her cellphone while gliding slowly down the street .  Length -  14\n",
            "\n",
            "German -  eine frau mit einer großen geldbörse geht an einem tor vorbei .  Length -  12\n",
            "English -  a woman with a large purse is walking by a gate .  Length -  12\n",
            "\n",
            "German -  jungen tanzen mitten in der nacht auf pfosten .  Length -  9\n",
            "English -  boys dancing on poles in the middle of the night .  Length -  11\n",
            "\n",
            "Maximum Length of English sentence 41 and German sentence 44 in the dataset\n",
            "Minimum Length of English sentence 4 and German sentence 1 in the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE_S5yMdwRsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5035c84c-33d9-44e1-a721-c17e52e33469"
      },
      "source": [
        "count = 0\n",
        "for data in train_iterator:\n",
        "  if count < 1 :\n",
        "    print(\"Shapes\", data.src.shape, data.trg.shape)\n",
        "    print()\n",
        "    print(\"German - \",*data.src, \" Length - \", len(data.src))\n",
        "    print()\n",
        "    print(\"English - \",*data.trg, \" Length - \", len(data.trg))\n",
        "    temp_ger = data.src\n",
        "    temp_eng = data.trg\n",
        "    count += 1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shapes torch.Size([18, 32]) torch.Size([20, 32])\n",
            "\n",
            "German -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([ 16,   4,  16,   4,   4,   4,  45, 249,   4,   4,  21,   4,  16,   4,\n",
            "          4,   4,   4,   4,  16,   4,   4,   4,   4,   4,   4,   4,  28,   4,\n",
            "          4,   4,  45,   4], device='cuda:0') tensor([ 30,  23, 632,  26,  15, 682,  61, 123, 119, 301, 229,   9,  49,   9,\n",
            "         15,  23,  23, 158,  19,  26,  55,  23,  15,  23,   9,  15, 264,   9,\n",
            "          9,   9,   9,  29], device='cuda:0') tensor([  17,   34,   17,   11,  148,   51,  107,  433,  237,   61,    9,    6,\n",
            "           6,  205,   10,   32,    9, 2454,   17,   35,   10,   15,  186,  528,\n",
            "          88,    6,   68,    6,   13,   98,    6,  175], device='cuda:0') tensor([  77, 1506,  611,   24,    4,   34,    0,   16,    6, 1382,  371, 1046,\n",
            "          85,    8,   31,   22,   10,   10,   31,  501,   36,   13,   10,   32,\n",
            "          79,    4,    7,    4,    4,    6, 1444, 1127], device='cuda:0') tensor([  40,   27,  230,   35,  619,  136,   20,   12,   60,   22,    6,  145,\n",
            "         266,    7,   70,  258, 2189,   44,   81,   21,   48,    4,  706,   11,\n",
            "           4,  588,  214,  217,    0,   42,   11,   40], device='cuda:0') tensor([   7,  137,  163,    6,    8,    4,    7,  152,   11,    4,    4,   11,\n",
            "          52,  187,   18,  213,   82,    4,    4,   85,    4,  130,   43,  934,\n",
            "           0,   10,    7, 4493,  155,   12,   45,    4], device='cuda:0') tensor([  39,   14,   71,    4,   43, 1762,   71,  410,   26, 2172,  246,    4,\n",
            "          18,   84,    4,  577,   15,  177,  157,   66,  159,   25,  413, 3886,\n",
            "         200,   36,    9,  136, 3442,    4,    6, 2254], device='cuda:0') tensor([   8,   57,    8,  378,  168,   28,   35,    4,  331,   88,  240,  892,\n",
            "         877,  148,  528, 3468,   13, 2454,   11,   14,   13,   10,   28, 2431,\n",
            "          14,    6,    6,   21,  928,  300,  721,  522], device='cuda:0') tensor([   4,   71,    4,  188,   10,   16,   28,  259,  429, 2889,   28,   67,\n",
            "           4,    7,   98,   18,    4,    6,   45,   41,    4,   54,  484,   65,\n",
            "          57,   47,    7, 1787,   18,  105,  398,   13], device='cuda:0') tensor([ 153,   23,   84,   10,   41,   30,  238,  356,   53,  230,   82,   10,\n",
            "         402,   66,    6, 2279,  601,    7,    9,    8,  350,   20,   43,    0,\n",
            "           4,   14,   26,  380,  864,   13,   53,   33], device='cuda:0') tensor([  70,  190,    6,  148, 1303,   17,  400,   74,    4,    4,   15,  374,\n",
            "         555,    6,   42,    4,   13,  101,   10,    7,   18,    4,  863,   18,\n",
            "         415,   11,   25,   28,   21,    7,    7,   14], device='cuda:0') tensor([  18, 1052,    4,    4,   12,   31,   17,    6,  131,   15,   90,  111,\n",
            "          20,   42,   12,  528,   46,   20,  243,  179, 1043,   61,  475,  149,\n",
            "         233,   10,  134,   54, 2404,   99,  115,   85], device='cuda:0') tensor([   4,    6,  133,  257,    4,    6,    6,   42,    8,   13,   70,   12,\n",
            "           4,   12,    4,    8, 1460,    4,   48,  248,   74,  312,  141,   40,\n",
            "         801,  328,   21,   18,  128,    6,    6,   11], device='cuda:0') tensor([ 130,    7,   12,  202,   55,    7,    7,   12,    7,   33,   18,    7,\n",
            "         402,    7, 2535,   43,    8,  187,    7,    6,    7,   12,   10,    7,\n",
            "          92,   48, 1104,    7,   13,    7,    4,   24], device='cuda:0') tensor([  76,  182,  119,  334,   56,   95,   96,  441,   84, 2783,  139,  386,\n",
            "         630, 3627,  232,  137,  106,  133,  157,  510,  159, 1568,  164, 1597,\n",
            "         139,  437,  122,  497, 2117,   95,   99,    0], device='cuda:0') tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5], device='cuda:0') tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')  Length -  18\n",
            "\n",
            "English -  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') tensor([ 16,   4,  16,   4,   4,   4,  46, 251,   4,   4,  21,   4,  16,   4,\n",
            "          4,   4,   4,   4,  16,   4,   4,   4,   4,   4,   4,   4,  28,   4,\n",
            "          4,   4,  46,   4], device='cuda:0') tensor([ 30,  24, 639,  26,  14, 683,  61, 127, 123, 299, 233,   9,  50,   9,\n",
            "         14,  24,  24, 161,  19,  26,  55,  24,  14,  24,   9,  14, 269,   9,\n",
            "          9,   9,   9,  29], device='cuda:0') tensor([  17,   34,   17,   11,  151,   53,  112,  439,  237,   61,    9,    6,\n",
            "           6,  212,   10,   33,    9, 2452,   17,   35,   10,   14,  189,  535,\n",
            "          89,    6,   69,    6,   13,  100,    6,  180], device='cuda:0') tensor([  78, 1518,  617,   25,    4,   34,    0,   16,    6, 1387,  375, 1049,\n",
            "          86,    8,   32,   22,   10,   10,   32,  504,   36,   13,   10,   33,\n",
            "          80,    4,    7,    4,    4,    6, 1454, 1128], device='cuda:0') tensor([  40,   27,  236,   35,  615,  140,   20,   12,   62,   22,    6,  148,\n",
            "         270,    7,   71,  263, 2190,   45,   83,   21,   49,    4,  714,   11,\n",
            "           4,  590,  220,  178,    0,   43,   11,   40], device='cuda:0') tensor([  7, 139, 167,   6,   8,   4,   7, 155,  11,   4,   4,  11,  51, 192,\n",
            "         18, 219,  82,   4,   4,  86,   4, 132,  44, 941,   0,  10,   7, 314,\n",
            "        158,  12,  46,   4], device='cuda:0') tensor([  39,   15,   72,    4,   44, 1768,   72,  414,   26, 2173,  253,    4,\n",
            "          18,   85,    4,  579,   14,  183,  159,   68,  162,   23,  418, 3872,\n",
            "         206,   36,    9,   42, 3433,    4,    6, 2265], device='cuda:0') tensor([   8,   58,    8,  384,  172,   28,   35,    4,  338,   89,  243,  898,\n",
            "         849,  151,  535, 3460,   13, 2452,   11,   15,   13,   10,   28, 2426,\n",
            "          15,    6,    6,  109,  933,  291,  728,  474], device='cuda:0') tensor([   4,   72,    4,  193,   10,   16,   28,  264,  436, 2893,   28,   67,\n",
            "           4,    7,  100,   18,    4,    6,   46,   41,    4,   56,  489,   66,\n",
            "          58,   47,    7,  140,   18,  108,  403,   13], device='cuda:0') tensor([ 157,   24,   85,   10,   41,   30,  244,  361,   54,  236,   82,   10,\n",
            "         408,   68,    6, 2289,  604,    7,    9,    8,  350,   20,   44,    0,\n",
            "           4,   15,   26,   21,  869,   13,   54,   31], device='cuda:0') tensor([  71,  196,    6,  151, 1305,   17,  405,   75,    4,    4,   14,  377,\n",
            "         551,    6,   43,    4,   13,  103,   10,    7,   18,    4,  829,   18,\n",
            "         420,   11,   23, 1789,   21,    7,    7,   15], device='cuda:0') tensor([  18, 1056,    4,    4,   12,   32,   17,    6,  134,   14,   91,  111,\n",
            "          20,   43,   12,  535,   48,   20,  249,  185, 1046,   61,  459,  152,\n",
            "         238,   10,  137,  379, 2403,  101,  116,   86], device='cuda:0') tensor([   4,    6,  135,  262,    4,    6,    6,   43,    8,   13,   71,   12,\n",
            "           4,   12,    4,    8, 1472,    4,   49,  222,   75,  315,  141,   40,\n",
            "         804,  334,   21,   28,  131,    6,    6,   11], device='cuda:0') tensor([ 132,    7,   12,  163,   55,    7,    7,   12,    7,   31,   18,    7,\n",
            "         408,    7, 2528,   44,    8,  192,    7,    6,    7,   12,   10,    7,\n",
            "          93,   49, 1106,   56,   13,    7,    4,   25], device='cuda:0') tensor([  77,  182,  123,  337,   57,   98,   95,  443,   85, 2787,  143,  392,\n",
            "         638, 3617,  102,  139,  109,  135,  159,  513,  162, 1579,  169, 1609,\n",
            "         143,  442,  126,   18, 2115,   98,  101,    0], device='cuda:0') tensor([  5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
            "        235,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   7,\n",
            "          5,   5,   5,   5], device='cuda:0') tensor([  3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
            "          5,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3, 500,\n",
            "          3,   3,   3,   3], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 5, 1, 1, 1, 1], device='cuda:0') tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 3, 1, 1, 1, 1], device='cuda:0')  Length -  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSP5RchXyuaz"
      },
      "source": [
        "temp_eng_idx = (temp_eng).cpu().detach().numpy()\n",
        "temp_ger_idx = (temp_ger).cpu().detach().numpy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgAmQS4I6k9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "6429d13f-cae9-42a5-cdee-a1b7e1e8fb5d"
      },
      "source": [
        "\n",
        "df_eng_idx = pd.DataFrame(data = temp_eng_idx, columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\n",
        "df_eng_idx.index.name = 'Time Steps'\n",
        "df_eng_idx.index = df_eng_idx.index + 1 \n",
        "# df_eng_idx.to_csv('/content/idx.csv')\n",
        "df_eng_idx\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>S_5</th>\n",
              "      <th>S_6</th>\n",
              "      <th>S_7</th>\n",
              "      <th>S_8</th>\n",
              "      <th>S_9</th>\n",
              "      <th>S_10</th>\n",
              "      <th>S_11</th>\n",
              "      <th>S_12</th>\n",
              "      <th>S_13</th>\n",
              "      <th>S_14</th>\n",
              "      <th>S_15</th>\n",
              "      <th>S_16</th>\n",
              "      <th>S_17</th>\n",
              "      <th>S_18</th>\n",
              "      <th>S_19</th>\n",
              "      <th>S_20</th>\n",
              "      <th>S_21</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>S_27</th>\n",
              "      <th>S_28</th>\n",
              "      <th>S_29</th>\n",
              "      <th>S_30</th>\n",
              "      <th>S_31</th>\n",
              "      <th>S_32</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time Steps</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>46</td>\n",
              "      <td>251</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>24</td>\n",
              "      <td>639</td>\n",
              "      <td>26</td>\n",
              "      <td>14</td>\n",
              "      <td>683</td>\n",
              "      <td>61</td>\n",
              "      <td>127</td>\n",
              "      <td>123</td>\n",
              "      <td>299</td>\n",
              "      <td>233</td>\n",
              "      <td>9</td>\n",
              "      <td>50</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>161</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>55</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>269</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17</td>\n",
              "      <td>34</td>\n",
              "      <td>17</td>\n",
              "      <td>11</td>\n",
              "      <td>151</td>\n",
              "      <td>53</td>\n",
              "      <td>112</td>\n",
              "      <td>439</td>\n",
              "      <td>237</td>\n",
              "      <td>61</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>212</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "      <td>9</td>\n",
              "      <td>2452</td>\n",
              "      <td>17</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "      <td>189</td>\n",
              "      <td>535</td>\n",
              "      <td>89</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>100</td>\n",
              "      <td>6</td>\n",
              "      <td>180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>78</td>\n",
              "      <td>1518</td>\n",
              "      <td>617</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "      <td>1387</td>\n",
              "      <td>375</td>\n",
              "      <td>1049</td>\n",
              "      <td>86</td>\n",
              "      <td>8</td>\n",
              "      <td>32</td>\n",
              "      <td>22</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>32</td>\n",
              "      <td>504</td>\n",
              "      <td>36</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "      <td>80</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1454</td>\n",
              "      <td>1128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>40</td>\n",
              "      <td>27</td>\n",
              "      <td>236</td>\n",
              "      <td>35</td>\n",
              "      <td>615</td>\n",
              "      <td>140</td>\n",
              "      <td>20</td>\n",
              "      <td>12</td>\n",
              "      <td>62</td>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>270</td>\n",
              "      <td>7</td>\n",
              "      <td>71</td>\n",
              "      <td>263</td>\n",
              "      <td>2190</td>\n",
              "      <td>45</td>\n",
              "      <td>83</td>\n",
              "      <td>21</td>\n",
              "      <td>49</td>\n",
              "      <td>4</td>\n",
              "      <td>714</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>590</td>\n",
              "      <td>220</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>11</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>139</td>\n",
              "      <td>167</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>155</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>51</td>\n",
              "      <td>192</td>\n",
              "      <td>18</td>\n",
              "      <td>219</td>\n",
              "      <td>82</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>86</td>\n",
              "      <td>4</td>\n",
              "      <td>132</td>\n",
              "      <td>44</td>\n",
              "      <td>941</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>314</td>\n",
              "      <td>158</td>\n",
              "      <td>12</td>\n",
              "      <td>46</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>39</td>\n",
              "      <td>15</td>\n",
              "      <td>72</td>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "      <td>1768</td>\n",
              "      <td>72</td>\n",
              "      <td>414</td>\n",
              "      <td>26</td>\n",
              "      <td>2173</td>\n",
              "      <td>253</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>85</td>\n",
              "      <td>4</td>\n",
              "      <td>579</td>\n",
              "      <td>14</td>\n",
              "      <td>183</td>\n",
              "      <td>159</td>\n",
              "      <td>68</td>\n",
              "      <td>162</td>\n",
              "      <td>23</td>\n",
              "      <td>418</td>\n",
              "      <td>3872</td>\n",
              "      <td>206</td>\n",
              "      <td>36</td>\n",
              "      <td>9</td>\n",
              "      <td>42</td>\n",
              "      <td>3433</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>58</td>\n",
              "      <td>8</td>\n",
              "      <td>384</td>\n",
              "      <td>172</td>\n",
              "      <td>28</td>\n",
              "      <td>35</td>\n",
              "      <td>4</td>\n",
              "      <td>338</td>\n",
              "      <td>89</td>\n",
              "      <td>243</td>\n",
              "      <td>898</td>\n",
              "      <td>849</td>\n",
              "      <td>151</td>\n",
              "      <td>535</td>\n",
              "      <td>3460</td>\n",
              "      <td>13</td>\n",
              "      <td>2452</td>\n",
              "      <td>11</td>\n",
              "      <td>15</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>28</td>\n",
              "      <td>2426</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>109</td>\n",
              "      <td>933</td>\n",
              "      <td>291</td>\n",
              "      <td>728</td>\n",
              "      <td>474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>72</td>\n",
              "      <td>4</td>\n",
              "      <td>193</td>\n",
              "      <td>10</td>\n",
              "      <td>16</td>\n",
              "      <td>28</td>\n",
              "      <td>264</td>\n",
              "      <td>436</td>\n",
              "      <td>2893</td>\n",
              "      <td>28</td>\n",
              "      <td>67</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>100</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>46</td>\n",
              "      <td>41</td>\n",
              "      <td>4</td>\n",
              "      <td>56</td>\n",
              "      <td>489</td>\n",
              "      <td>66</td>\n",
              "      <td>58</td>\n",
              "      <td>47</td>\n",
              "      <td>7</td>\n",
              "      <td>140</td>\n",
              "      <td>18</td>\n",
              "      <td>108</td>\n",
              "      <td>403</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>157</td>\n",
              "      <td>24</td>\n",
              "      <td>85</td>\n",
              "      <td>10</td>\n",
              "      <td>41</td>\n",
              "      <td>30</td>\n",
              "      <td>244</td>\n",
              "      <td>361</td>\n",
              "      <td>54</td>\n",
              "      <td>236</td>\n",
              "      <td>82</td>\n",
              "      <td>10</td>\n",
              "      <td>408</td>\n",
              "      <td>68</td>\n",
              "      <td>6</td>\n",
              "      <td>2289</td>\n",
              "      <td>604</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>350</td>\n",
              "      <td>20</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>21</td>\n",
              "      <td>869</td>\n",
              "      <td>13</td>\n",
              "      <td>54</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>71</td>\n",
              "      <td>196</td>\n",
              "      <td>6</td>\n",
              "      <td>151</td>\n",
              "      <td>1305</td>\n",
              "      <td>17</td>\n",
              "      <td>405</td>\n",
              "      <td>75</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>377</td>\n",
              "      <td>551</td>\n",
              "      <td>6</td>\n",
              "      <td>43</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>103</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>4</td>\n",
              "      <td>829</td>\n",
              "      <td>18</td>\n",
              "      <td>420</td>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>1789</td>\n",
              "      <td>21</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>18</td>\n",
              "      <td>1056</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>32</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>134</td>\n",
              "      <td>14</td>\n",
              "      <td>91</td>\n",
              "      <td>111</td>\n",
              "      <td>20</td>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>535</td>\n",
              "      <td>48</td>\n",
              "      <td>20</td>\n",
              "      <td>249</td>\n",
              "      <td>185</td>\n",
              "      <td>1046</td>\n",
              "      <td>61</td>\n",
              "      <td>459</td>\n",
              "      <td>152</td>\n",
              "      <td>238</td>\n",
              "      <td>10</td>\n",
              "      <td>137</td>\n",
              "      <td>379</td>\n",
              "      <td>2403</td>\n",
              "      <td>101</td>\n",
              "      <td>116</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>135</td>\n",
              "      <td>262</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>43</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>71</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1472</td>\n",
              "      <td>4</td>\n",
              "      <td>49</td>\n",
              "      <td>222</td>\n",
              "      <td>75</td>\n",
              "      <td>315</td>\n",
              "      <td>141</td>\n",
              "      <td>40</td>\n",
              "      <td>804</td>\n",
              "      <td>334</td>\n",
              "      <td>21</td>\n",
              "      <td>28</td>\n",
              "      <td>131</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>132</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>163</td>\n",
              "      <td>55</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>31</td>\n",
              "      <td>18</td>\n",
              "      <td>7</td>\n",
              "      <td>408</td>\n",
              "      <td>7</td>\n",
              "      <td>2528</td>\n",
              "      <td>44</td>\n",
              "      <td>8</td>\n",
              "      <td>192</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>93</td>\n",
              "      <td>49</td>\n",
              "      <td>1106</td>\n",
              "      <td>56</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>77</td>\n",
              "      <td>182</td>\n",
              "      <td>123</td>\n",
              "      <td>337</td>\n",
              "      <td>57</td>\n",
              "      <td>98</td>\n",
              "      <td>95</td>\n",
              "      <td>443</td>\n",
              "      <td>85</td>\n",
              "      <td>2787</td>\n",
              "      <td>143</td>\n",
              "      <td>392</td>\n",
              "      <td>638</td>\n",
              "      <td>3617</td>\n",
              "      <td>102</td>\n",
              "      <td>139</td>\n",
              "      <td>109</td>\n",
              "      <td>135</td>\n",
              "      <td>159</td>\n",
              "      <td>513</td>\n",
              "      <td>162</td>\n",
              "      <td>1579</td>\n",
              "      <td>169</td>\n",
              "      <td>1609</td>\n",
              "      <td>143</td>\n",
              "      <td>442</td>\n",
              "      <td>126</td>\n",
              "      <td>18</td>\n",
              "      <td>2115</td>\n",
              "      <td>98</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>235</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>500</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            S_1   S_2  S_3  S_4   S_5   S_6  ...  S_27  S_28  S_29  S_30  S_31  S_32\n",
              "Time Steps                                   ...                                    \n",
              "1             2     2    2    2     2     2  ...     2     2     2     2     2     2\n",
              "2            16     4   16    4     4     4  ...    28     4     4     4    46     4\n",
              "3            30    24  639   26    14   683  ...   269     9     9     9     9    29\n",
              "4            17    34   17   11   151    53  ...    69     6    13   100     6   180\n",
              "5            78  1518  617   25     4    34  ...     7     4     4     6  1454  1128\n",
              "6            40    27  236   35   615   140  ...   220   178     0    43    11    40\n",
              "7             7   139  167    6     8     4  ...     7   314   158    12    46     4\n",
              "8            39    15   72    4    44  1768  ...     9    42  3433     4     6  2265\n",
              "9             8    58    8  384   172    28  ...     6   109   933   291   728   474\n",
              "10            4    72    4  193    10    16  ...     7   140    18   108   403    13\n",
              "11          157    24   85   10    41    30  ...    26    21   869    13    54    31\n",
              "12           71   196    6  151  1305    17  ...    23  1789    21     7     7    15\n",
              "13           18  1056    4    4    12    32  ...   137   379  2403   101   116    86\n",
              "14            4     6  135  262     4     6  ...    21    28   131     6     6    11\n",
              "15          132     7   12  163    55     7  ...  1106    56    13     7     4    25\n",
              "16           77   182  123  337    57    98  ...   126    18  2115    98   101     0\n",
              "17            5     5    5    5     5     5  ...     5     7     5     5     5     5\n",
              "18            3     3    3    3     3     3  ...     3   500     3     3     3     3\n",
              "19            1     1    1    1     1     1  ...     1     5     1     1     1     1\n",
              "20            1     1    1    1     1     1  ...     1     3     1     1     1     1\n",
              "\n",
              "[20 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXy1431M6o02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "84f744c8-f461-4bbf-a6e7-c97b1945bbe4"
      },
      "source": [
        "df_eng_word = pd.DataFrame(columns = [str(\"S_\")+str(x) for x in np.arange(1, 33)])\n",
        "df_eng_word = df_eng_idx.replace(idx_2_word)\n",
        "# df_eng_word.to_csv('/content/Words.csv')\n",
        "df_eng_word"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-05f2162adee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_eng_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_eng_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_2_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# df_eng_word.to_csv('/content/Words.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_eng_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'idx_2_word' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLV-t-MzVQdg"
      },
      "source": [
        "## EncoderLSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dZT3Zs17yMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb55d923-34ae-42c2-82a9-629f4f1ebc24"
      },
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.tag = True\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    \n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "  # Shape of x (26, 32) [Sequence_length, batch_size]\n",
        "  def forward(self, x):\n",
        "\n",
        "    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    \n",
        "    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
        "\n",
        "    return hidden_state, cell_state\n",
        "\n",
        "input_size_encoder = len(german.vocab)\n",
        "encoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "encoder_dropout = 0.5\n",
        "\n",
        "encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
        "                           hidden_size, num_layers, encoder_dropout).to(device)\n",
        "print(encoder_lstm)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(5374, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTew1tbHVer5"
      },
      "source": [
        "## DecoderLSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGbQiBP72iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a50fdc3-baef-4177-9d62-388a1c1ca411"
      },
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "\n",
        "    # Size of the one hot vectors that will be the input to the encoder\n",
        "    #self.input_size = input_size\n",
        "\n",
        "    # Output size of the word embedding NN\n",
        "    #self.embedding_size = embedding_size\n",
        "\n",
        "    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Number of layers in the lstm\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Regularization parameter\n",
        "    self.dropout = nn.Dropout(p)\n",
        "\n",
        "    # Shape --------------------> (5376, 300) [input size, embedding dims]\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "\n",
        "    # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n",
        "    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "\n",
        "    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  # Shape of x (32) [batch_size]\n",
        "  def forward(self, x, hidden_state, cell_state):\n",
        "\n",
        "    # Shape of x (1, 32) [1, batch_size]\n",
        "    x = x.unsqueeze(0)\n",
        "\n",
        "    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "\n",
        "    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n",
        "    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
        "\n",
        "    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n",
        "    predictions = self.fc(outputs)\n",
        "\n",
        "    # Shape --> predictions (32, 4556) [batch_size , output_size]\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden_state, cell_state\n",
        "\n",
        "input_size_decoder = len(english.vocab)\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "decoder_dropout = 0.5\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
        "                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
        "print(decoder_lstm)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecoderLSTM(\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (embedding): Embedding(4556, 300)\n",
            "  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=4556, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xof3dPly753w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8faf86-b075-4a5d-e28a-e9832643f735"
      },
      "source": [
        "for batch in train_iterator:\n",
        "  print(batch.src.shape)\n",
        "  print(batch.trg.shape)\n",
        "  break\n",
        "\n",
        "x = batch.trg[1]\n",
        "print(x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([37, 32])\n",
            "torch.Size([37, 32])\n",
            "tensor([  16,   48,    4,    4,    4,   48,    4,    4,    4,  176,    4,    4,\n",
            "           4,   16,    4,    4,    4,    4,  176,    4,    4,   24,    4,    4,\n",
            "          21,    4, 1720,    4,    4,    4,    4,    4], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGnQbCnGVire"
      },
      "source": [
        "# Sequence to Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vzOor_Q782h"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.Encoder_LSTM = Encoder_LSTM\n",
        "    self.Decoder_LSTM = Decoder_LSTM\n",
        "\n",
        "  def forward(self, source, target, tfr=0.5):\n",
        "    # Shape - Source : (10, 32) [(Sentence length german + some padding), Number of Sentences]\n",
        "    batch_size = source.shape[1]\n",
        "\n",
        "    # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english.vocab)\n",
        "    \n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n",
        "    hidden_state, cell_state = self.Encoder_LSTM(source)\n",
        "\n",
        "    # Shape of x (32 elements)\n",
        "    x = target[0] # Trigger token <SOS>\n",
        "\n",
        "    for i in range(1, target_len):\n",
        "      # Shape --> output (32, 5766) \n",
        "      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
        "      outputs[i] = output\n",
        "      best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
        "      x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
        "\n",
        "    # Shape --> outputs (14, 32, 5766) \n",
        "    return outputs\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywW6f9fM8AMa"
      },
      "source": [
        "# Hyperparameters\n",
        "\n",
        "learning_rate = 0.001\n",
        "writer = SummaryWriter(f\"runs/loss_plot\")\n",
        "step = 0\n",
        "\n",
        "model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD0pRilG8CHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b5df9d-9815-41d5-f0c8-bd80cb3f5417"
      },
      "source": [
        "model"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (Encoder_LSTM): EncoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(5374, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (Decoder_LSTM): DecoderLSTM(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(4556, 300)\n",
              "    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=4556, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQyZ_vfq8G6C"
      },
      "source": [
        "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
        "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
        "\n",
        "    if type(sentence) == str:\n",
        "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "    tokens.insert(0, german.init_token)\n",
        "    tokens.append(german.eos_token)\n",
        "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Build encoder hidden, cell state\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n",
        "            best_guess = output.argmax(1).item()\n",
        "\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Model predicts it's the end of the sentence\n",
        "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    return translated_sentence[1:]\n",
        "\n",
        "def bleu(data, model, german, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, german, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n",
        "    print('saving')\n",
        "    print()\n",
        "    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n",
        "    torch.save(state, './checkpoint-NMT')\n",
        "    torch.save(model.state_dict(),'./checkpoint-NMT-SD')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysc4A5HX8Qyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89034a25-128b-4570-d72d-45f61710aa18"
      },
      "source": [
        "epoch_loss = 0.0\n",
        "num_epochs = 100\n",
        "best_loss = 999999\n",
        "best_epoch = -1\n",
        "sentence1 = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n",
        "ts1  = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n",
        "  model.eval()\n",
        "  translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=50)\n",
        "  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n",
        "  ts1.append(translated_sentence1)\n",
        "\n",
        "  model.train(True)\n",
        "  for batch_idx, batch in enumerate(train_iterator):\n",
        "    input = batch.src.to(device)\n",
        "    target = batch.trg.to(device)\n",
        "\n",
        "    # Pass the input and target for model's forward method\n",
        "    output = model(input, target)\n",
        "    output = output[1:].reshape(-1, output.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "\n",
        "    # Clear the accumulating gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate the loss value for every epoch\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    # Calculate the gradients for weights & biases using back-propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient value is it exceeds > 1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "    # Update the weights values using the gradients we calculated using bp \n",
        "    optimizer.step()\n",
        "    step += 1\n",
        "    epoch_loss += loss.item()\n",
        "    writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
        "\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    best_epoch = epoch\n",
        "    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n",
        "    if ((epoch - best_epoch) >= 10):\n",
        "      print(\"no improvement in 10 epochs, break\")\n",
        "      break\n",
        "  print(\"Epoch_Loss - {}\".format(loss.item()))\n",
        "  print()\n",
        "  \n",
        "print(epoch_loss / len(train_iterator))\n",
        "\n",
        "# score = bleu(test_data[1:100], model, german, english, device)\n",
        "# print(f\"Bleu score {score*100:.2f}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch - 1 / 100\n",
            "Translated example sentence 1: \n",
            " ['sit', 'moving', 'moving', 'bungee', 'drivers', 'force', 'moped', 'gloved', 'hammock', 'gloved', 'faced', 'faced', 'itself', 'hammock', 'daylight', 'daylight', 'meditating', 'meditating', 'meditating', 'gloved', 'curly', 'curly', 'gloved', 'gloved', 'hammock', 'gloved', 'hotdog', 'gloved', 'hotdog', 'rolled', 'rolled', 'rolled', 'hairnet', 'moped', 'designer', 'designer', 'designer', 'unloading', 'elbow', 'darkened', 'ally', 'artist', 'rolled', 'rolled', 'moped', 'moped', 'remote', 'remote', 'feet', 'feet']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving\n",
            "\n",
            "Epoch_Loss - 2.6146411895751953\n",
            "\n",
            "Epoch - 2 / 100\n",
            "Translated example sentence 1: \n",
            " ['there', 'in', 'the', 'air', 'with', '<unk>', ',', ',', 'and', 'the', '<unk>', 'and', '<unk>', '.', '<eos>']\n",
            "Epoch_Loss - 3.4478724002838135\n",
            "\n",
            "Epoch - 3 / 100\n",
            "Translated example sentence 1: \n",
            " ['here', 'with', 'the', 'ball', 'in', 'the', 'and', 'is', 'is', 'is', 'to', 'to', 'it', '.', '<eos>']\n",
            "Epoch_Loss - 1.9869029521942139\n",
            "\n",
            "Epoch - 4 / 100\n",
            "Translated example sentence 1: \n",
            " ['cheerleaders', 'with', 'the', 'field', 'in', 'the', ',', ',', 'is', 'is', 'is', 'is', 'up', '.', '<eos>']\n",
            "Epoch_Loss - 1.5970442295074463\n",
            "\n",
            "Epoch - 5 / 100\n",
            "Translated example sentence 1: \n",
            " ['here', 'with', 'the', 'street', 'in', 'his', ',', 'and', 'is', 'is', 'is', 'his', 'them', '.', '<eos>']\n",
            "Epoch_Loss - 1.3171850442886353\n",
            "\n",
            "Epoch - 6 / 100\n",
            "Translated example sentence 1: \n",
            " ['worker', 'with', 'the', 'number', 'on', 'top', ',', 'black', 'and', 'is', 'is', 'his', 'them', '.', '<eos>']\n",
            "Epoch_Loss - 1.2669719457626343\n",
            "\n",
            "Epoch - 7 / 100\n",
            "Translated example sentence 1: \n",
            " ['workmen', 'in', 'the', 'street', 'with', 'with', 'blue', 'and', 'and', 'is', 'is', 'looking', 'them', '.', '<eos>']\n",
            "Epoch_Loss - 1.4191961288452148\n",
            "\n",
            "Epoch - 8 / 100\n",
            "Translated example sentence 1: \n",
            " ['painters', 'in', 'the', 'city', 'park', 'with', 'black', ',', 'and', 'is', 'at', 'his', 'holes', '.', '<eos>']\n",
            "Epoch_Loss - 2.1685597896575928\n",
            "\n",
            "Epoch - 9 / 100\n",
            "Translated example sentence 1: \n",
            " ['painters', 'in', 'the', 'city', 'area', 'with', 'black', ',', 'and', 'is', 'is', 'rock', 'something', '.', '<eos>']\n",
            "Epoch_Loss - 1.8665292263031006\n",
            "\n",
            "Epoch - 10 / 100\n",
            "Translated example sentence 1: \n",
            " ['musician', 'with', 'the', 'city', 'in', 'this', 'black', ',', ',', 'is', 'is', 'is', 'skiing', '.', '<eos>']\n",
            "Epoch_Loss - 1.1433619260787964\n",
            "\n",
            "Epoch - 11 / 100\n",
            "Translated example sentence 1: \n",
            " ['gentlemen', 'with', 'the', 'city', 'on', 'this', 'black', ',', 'and', 'is', 'is', 'in', 'pain', '.', '<eos>']\n",
            "Epoch_Loss - 0.5652026534080505\n",
            "\n",
            "Epoch - 12 / 100\n",
            "Translated example sentence 1: \n",
            " ['musician', 'with', 'the', 'city', 'city', 'in', 'blue', 'black', ',', 'is', 'looking', 'very', 'vegetables', '.', '<eos>']\n",
            "Epoch_Loss - 0.9093621969223022\n",
            "\n",
            "Epoch - 13 / 100\n",
            "Translated example sentence 1: \n",
            " ['cityscape', 'with', 'the', 'beautiful', 'city', 'in', 'blue', 'and', 'blue', ',', 'is', 'is', 'alone', '.', '<eos>']\n",
            "Epoch_Loss - 0.8550674319267273\n",
            "\n",
            "Epoch - 14 / 100\n",
            "Translated example sentence 1: \n",
            " ['cityscape', 'with', 'the', 'beautiful', 'streets', 'in', 'black', 'and', 'gold', 'is', 'looking', 'art', '.', '<eos>']\n",
            "Epoch_Loss - 0.3419819474220276\n",
            "\n",
            "Epoch - 15 / 100\n",
            "Translated example sentence 1: \n",
            " ['reflections', 'with', 'the', 'beautiful', 'full', 'in', 'black', ',', 'and', 'is', 'is', 'reading', 'attention', '.', '<eos>']\n",
            "Epoch_Loss - 0.26338011026382446\n",
            "\n",
            "Epoch - 16 / 100\n",
            "Translated example sentence 1: \n",
            " ['protesters', 'with', 'the', 'beautiful', 'full', 'of', 'blue', ',', 'and', 'is', 'is', 'operating', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 1.8032499551773071\n",
            "\n",
            "Epoch - 17 / 100\n",
            "Translated example sentence 1: \n",
            " ['painters', 'with', 'the', 'beautiful', 'full', 'in', 'blue', ',', 'white', 'is', 'eating', 'something', 'art', '.', '<eos>']\n",
            "Epoch_Loss - 0.944947361946106\n",
            "\n",
            "Epoch - 18 / 100\n",
            "Translated example sentence 1: \n",
            " ['reflections', 'with', 'the', 'foreground', 'in', 'in', 'blue', ',', 'white', 'is', 'is', 'looking', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.38484033942222595\n",
            "\n",
            "Epoch - 19 / 100\n",
            "Translated example sentence 1: \n",
            " ['cityscape', 'with', 'the', 'face', 'in', 'mostly', 'blue', 'shirt', ',', 'is', 'down', 'public', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 1.629515528678894\n",
            "\n",
            "Epoch - 20 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'night', 'in', 'this', 'blue', ',', 'and', 'he', 'is', 'climbing', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 1.7631244659423828\n",
            "\n",
            "Epoch - 21 / 100\n",
            "Translated example sentence 1: \n",
            " ['cityscape', 'with', 'the', 'streets', 'in', 'this', 'blue', 'black', 'striped', 'shirt', 'is', 'down', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.5561147332191467\n",
            "\n",
            "Epoch - 22 / 100\n",
            "Translated example sentence 1: \n",
            " ['reflections', 'with', 'the', 'unusual', 'on', 'in', 'blue', 'and', 'blue', ',', 'is', 'looking', 'forehead', '.', '<eos>']\n",
            "Epoch_Loss - 0.3315199017524719\n",
            "\n",
            "Epoch - 23 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'rocks', 'on', ',', 'blue', ',', 'white', ',', 'is', 'rock', 'forehead', '.', '<eos>']\n",
            "Epoch_Loss - 1.9243690967559814\n",
            "\n",
            "Epoch - 24 / 100\n",
            "Translated example sentence 1: \n",
            " ['painters', 'with', 'the', 'three', 'on', 'person', 'person', 'person', 'he', 'is', 'standing', 'in', 'tow', '.', '<eos>']\n",
            "Epoch_Loss - 0.12268977612257004\n",
            "\n",
            "Epoch - 25 / 100\n",
            "Translated example sentence 1: \n",
            " ['reflections', 'with', 'the', 'street', 'in', 'in', 'blue', 'overalls', 'and', 'is', 'coming', 'off', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.3354605734348297\n",
            "\n",
            "Epoch - 26 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'streets', 'on', 'snow', 'blue', 'white', ',', 'is', 'is', 'rock', 'climbing', '.', '<eos>']\n",
            "Epoch_Loss - 0.27141332626342773\n",
            "\n",
            "Epoch - 27 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'streets', 'covered', 'in', 'white', 'pants', ',', 'is', 'walking', 'through', 'stairs', '.', '<eos>']\n",
            "Epoch_Loss - 0.19859860837459564\n",
            "\n",
            "Epoch - 28 / 100\n",
            "Translated example sentence 1: \n",
            " ['cityscape', 'with', 'the', 'streets', 'in', 'this', 'blue', 'and', 'white', 'hat', 'is', 'down', 'art', '.', '<eos>']\n",
            "Epoch_Loss - 0.14293240010738373\n",
            "\n",
            "Epoch - 29 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'street', 'in', 'in', 'blue', 'and', 'white', 'shorts', 'is', 'falling', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 1.1028270721435547\n",
            "\n",
            "Epoch - 30 / 100\n",
            "Translated example sentence 1: \n",
            " ['cityscape', 'with', 'the', 'street', 'in', 'all', 'blue', 'and', 'white', 'sleeved', 'is', 'is', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.0834505707025528\n",
            "\n",
            "Epoch - 31 / 100\n",
            "Translated example sentence 1: \n",
            " ['with', 'with', 'the', 'face', 'in', 'in', 'blue', 'and', 'black', 'pants', 'is', 'sprinting', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.14010560512542725\n",
            "\n",
            "Epoch - 32 / 100\n",
            "Translated example sentence 1: \n",
            " ['with', 'with', 'the', 'three', 'in', 'blue', 'blue', 'blue', 'sleeved', 'sleeved', 'is', 'is', 'eating', '.', '<eos>']\n",
            "Epoch_Loss - 0.6971834301948547\n",
            "\n",
            "Epoch - 33 / 100\n",
            "Translated example sentence 1: \n",
            " ['hands', 'with', 'the', 'corner', 'of', 'leaves', 'blue', ',', 'white', 'shorts', 'is', 'looking', 'by', '.', '<eos>']\n",
            "Epoch_Loss - 0.15165063738822937\n",
            "\n",
            "Epoch - 34 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'street', 'in', 'this', 'blue', 'black', 'uniform', 'is', 'is', 'at', 'himself', '.', '<eos>']\n",
            "Epoch_Loss - 0.07683773338794708\n",
            "\n",
            "Epoch - 35 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'night', 'in', 'his', 'blue', 'hot', 'ears', 'is', 'taking', 'buy', 'stairs', '.', '<eos>']\n",
            "Epoch_Loss - 0.6078615188598633\n",
            "\n",
            "Epoch - 36 / 100\n",
            "Translated example sentence 1: \n",
            " ['well', 'with', 'the', 'focus', 'in', 'his', 'blue', 'shirt', ',', 'is', 'looking', 'at', 'fruit', '.', '<eos>']\n",
            "Epoch_Loss - 0.06587251275777817\n",
            "\n",
            "Epoch - 37 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'face', 'in', 'blue', 'blue', 'and', 'black', 'shorts', 'is', 'climbing', 'public', '.', '<eos>']\n",
            "Epoch_Loss - 0.06004098057746887\n",
            "\n",
            "Epoch - 38 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'face', 'in', 'those', 'blue', ',', 'red', 'is', 'is', 'indoor', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.12480513751506805\n",
            "\n",
            "Epoch - 39 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'streets', 'in', 'both', 'blue', 'color', ',', 'is', 'down', 'or', 'onlookers', '.', '<eos>']\n",
            "Epoch_Loss - 0.3743496239185333\n",
            "\n",
            "Epoch - 40 / 100\n",
            "Translated example sentence 1: \n",
            " ['workmen', 'with', 'the', 'policemen', 'in', 'down', 'blue', ',', 'who', 'is', 'hanging', 'or', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.11595889925956726\n",
            "\n",
            "Epoch - 41 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'face', 'in', 'in', 'blue', 'overalls', ',', 'is', 'is', 'climbing', 'ahead', '.', '<eos>']\n",
            "Epoch_Loss - 0.1662605106830597\n",
            "\n",
            "Epoch - 42 / 100\n",
            "Translated example sentence 1: \n",
            " ['painters', 'with', 'the', 'focus', 'in', 'his', 'blue', 'sleeved', 'sleeved', ',', 'is', 'looking', 'ahead', '.', '<eos>']\n",
            "Epoch_Loss - 0.9771254658699036\n",
            "\n",
            "Epoch - 43 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'painted', 'in', 'down', 'blue', 'pants', ',', 'is', 'taking', 'his', 'tickets', '.', '<eos>']\n",
            "Epoch_Loss - 0.9205565452575684\n",
            "\n",
            "Epoch - 44 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'driver', 'of', 'leaves', 'person', 'blue', 'is', 'down', 'down', 'in', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.09040384739637375\n",
            "\n",
            "Epoch - 45 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'focus', 'in', 'full', 'blue', 'and', 'blue', ',', 'is', 'down', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.06985635310411453\n",
            "\n",
            "Epoch - 46 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'face', 'of', 'those', 'blue', 'pants', ',', 'is', 'down', 'at', 'intersection', '.', '<eos>']\n",
            "Epoch_Loss - 0.06965994834899902\n",
            "\n",
            "Epoch - 47 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'focus', 'in', 'blue', 'blue', 'shirt', ',', 'is', 'down', 'is', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.3400649130344391\n",
            "\n",
            "Epoch - 48 / 100\n",
            "Translated example sentence 1: \n",
            " ['workmen', 'with', 'the', 'focus', 'in', 'in', 'blue', 'and', 'red', ',', 'is', 'down', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.11403832584619522\n",
            "\n",
            "Epoch - 49 / 100\n",
            "Translated example sentence 1: \n",
            " ['well', 'with', 'the', 'focus', 'in', 'full', 'blue', 'and', 'black', 'hat', 'is', 'down', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.0959254652261734\n",
            "\n",
            "Epoch - 50 / 100\n",
            "Translated example sentence 1: \n",
            " ['well', 'with', 'the', 'focus', 'in', 'in', 'blue', 'and', 'red', 'hat', 'is', 'looking', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.07316362857818604\n",
            "\n",
            "Epoch - 51 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'focus', 'in', 'full', 'blue', 'and', 'he', 'is', 'down', 'by', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.9726515412330627\n",
            "\n",
            "Epoch - 52 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'focus', 'in', 'one', 'of', 'blue', ',', 'is', 'looking', 'down', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.0877087414264679\n",
            "\n",
            "Epoch - 53 / 100\n",
            "Translated example sentence 1: \n",
            " ['painted', 'with', 'the', 'focus', 'in', 'in', 'blue', 'and', 'black', ',', 'is', 'at', 'night', '.', '<eos>']\n",
            "Epoch_Loss - 0.7411105632781982\n",
            "\n",
            "Epoch - 54 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'focus', 'in', 'snow', 'blue', 'and', 'black', 'is', 'taking', 'his', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.07184508442878723\n",
            "\n",
            "Epoch - 55 / 100\n",
            "Translated example sentence 1: \n",
            " ['everyone', 'with', 'the', 'focus', 'in', 'down', 'blue', 'and', 'is', 'is', 'climbing', 'close', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.07680891454219818\n",
            "\n",
            "Epoch - 56 / 100\n",
            "Translated example sentence 1: \n",
            " ['well', 'with', 'the', 'focus', 'in', 'down', 'blue', 'and', 'whom', 'is', 'looking', 'or', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.07573024183511734\n",
            "\n",
            "Epoch - 57 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'face', 'on', 'snow', 'of', 'whom', ',', 'is', 'doing', 'or', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.4583074152469635\n",
            "\n",
            "Epoch - 58 / 100\n",
            "Translated example sentence 1: \n",
            " ['reflections', 'with', 'the', 'crown', 'in', 'snow', 'of', 'whom', ',', 'whom', 'is', 'coming', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.07505790889263153\n",
            "\n",
            "Epoch - 59 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'night', 'on', 'blue', 'of', 'whom', ',', 'is', 'is', 'climbing', 'books', '.', '<eos>']\n",
            "Epoch_Loss - 0.2975274920463562\n",
            "\n",
            "Epoch - 60 / 100\n",
            "Translated example sentence 1: \n",
            " ['workmen', 'with', 'the', 'warm', 'in', 'in', 'blue', 'shirt', ',', 'is', 'is', 'moving', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.922227144241333\n",
            "\n",
            "Epoch - 61 / 100\n",
            "Translated example sentence 1: \n",
            " ['policemen', 'with', 'the', 'focus', 'in', 'all', 'blue', ',', ',', 'is', 'taking', 'his', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.09968975931406021\n",
            "\n",
            "Epoch - 62 / 100\n",
            "Translated example sentence 1: \n",
            " ['with', 'with', 'the', 'night', 'equipment', 'in', 'blue', ',', 'and', 'is', 'taking', 'at', 'beers', '.', '<eos>']\n",
            "Epoch_Loss - 0.14422214031219482\n",
            "\n",
            "Epoch - 63 / 100\n",
            "Translated example sentence 1: \n",
            " ['wall', 'with', 'the', 'line', 'of', 'snow', 'on', 'blue', ',', 'whom', 'is', 'bent', 'off', '.', '<eos>']\n",
            "Epoch_Loss - 0.08488261699676514\n",
            "\n",
            "Epoch - 64 / 100\n",
            "Translated example sentence 1: \n",
            " ['dressed', 'with', 'the', 'top', 'in', 'snow', 'blue', 'and', 'red', 'is', 'hanging', 'down', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.2523447275161743\n",
            "\n",
            "Epoch - 65 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'top', 'in', 'several', 'blue', 'and', 'red', 'is', 'is', 'down', 'moved', '.', '<eos>']\n",
            "Epoch_Loss - 0.5859572887420654\n",
            "\n",
            "Epoch - 66 / 100\n",
            "Translated example sentence 1: \n",
            " ['dressed', 'with', 'the', 'focus', 'in', 'this', 'blue', 'snow', ',', 'is', 'is', 'down', 'by', '.', '<eos>']\n",
            "Epoch_Loss - 0.1530437171459198\n",
            "\n",
            "Epoch - 67 / 100\n",
            "Translated example sentence 1: \n",
            " ['wall', 'with', 'the', 'focus', 'of', 'snow', 'in', 'snow', ',', 'is', 'being', 'down', 'ahead', '.', '<eos>']\n",
            "Epoch_Loss - 0.5558596849441528\n",
            "\n",
            "Epoch - 68 / 100\n",
            "Translated example sentence 1: \n",
            " ['dressed', 'with', 'the', 'focus', 'in', 'down', 'of', 'black', ',', 'is', 'being', 'balloon', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.5854015350341797\n",
            "\n",
            "Epoch - 69 / 100\n",
            "Translated example sentence 1: \n",
            " ['wall', 'with', 'the', 'focus', 'in', 'down', 'blue', ',', 'and', 'is', 'coming', 'down', 'ahead', '.', '<eos>']\n",
            "Epoch_Loss - 0.5483026504516602\n",
            "\n",
            "Epoch - 70 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'focus', 'in', 'snow', 'blue', ',', 'and', 'is', 'riding', 'his', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.8894839882850647\n",
            "\n",
            "Epoch - 71 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'focus', 'in', 'snow', 'blue', ',', 'and', 'is', 'riding', 'his', 'traffic', '.', '<eos>']\n",
            "Epoch_Loss - 0.06863036006689072\n",
            "\n",
            "Epoch - 72 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'face', 'in', 'snow', 'blue', ',', ',', 'is', 'is', 'climbing', 'night', '.', '<eos>']\n",
            "Epoch_Loss - 0.1524718850851059\n",
            "\n",
            "Epoch - 73 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'down', 'in', 'big', 'blue', 'shirt', 'is', 'is', 'hanging', 'by', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.891145646572113\n",
            "\n",
            "Epoch - 74 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'focus', 'in', 'one', 'blue', 'and', 'whom', 'is', 'coming', 'down', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.10580791532993317\n",
            "\n",
            "Epoch - 75 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'top', 'of', 'snow', 'blue', 'and', 'black', 'is', 'is', 'down', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.13330206274986267\n",
            "\n",
            "Epoch - 76 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'the', 'in', 'of', 'blue', 'shirt', 'is', 'is', 'down', 'down', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.0725429505109787\n",
            "\n",
            "Epoch - 77 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'on', 'on', 'top', 'of', 'snow', 'is', 'being', 'upside', 'down', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.1773262768983841\n",
            "\n",
            "Epoch - 78 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'top', 'in', 'snow', 'of', 'snow', 'is', 'hanging', 'down', 'by', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.1351011097431183\n",
            "\n",
            "Epoch - 79 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'focus', 'in', 'snow', 'blue', 'shorts', 'is', 'is', 'down', 'still', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.12460219860076904\n",
            "\n",
            "Epoch - 80 / 100\n",
            "Translated example sentence 1: \n",
            " ['workmen', 'with', 'the', 'face', 'in', 'burgundy', ',', 'blue', 'is', 'is', 'looking', 'at', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.07600365579128265\n",
            "\n",
            "Epoch - 81 / 100\n",
            "Translated example sentence 1: \n",
            " ['image', 'with', 'the', 'face', 'in', 'snow', 'of', \"'s\", ',', 'is', 'is', 'climbing', 'by', '.', '<eos>']\n",
            "Epoch_Loss - 0.11098877340555191\n",
            "\n",
            "Epoch - 82 / 100\n",
            "Translated example sentence 1: \n",
            " ['bicyclist', 'with', 'the', 'painted', 'in', 'burgundy', '<unk>', ',', 'is', 'is', 'coming', 'down', 'traffic', '.', '<eos>']\n",
            "Epoch_Loss - 0.044796593487262726\n",
            "\n",
            "Epoch - 83 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'painted', 'in', 'top', 'of', 'them', 'that', 'is', 'is', 'climbing', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.07194754481315613\n",
            "\n",
            "Epoch - 84 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'painted', 'in', 'burgundy', 'of', 'black', ',', 'is', 'is', 'down', 'down', '.', '<eos>']\n",
            "Epoch_Loss - 0.16728706657886505\n",
            "\n",
            "Epoch - 85 / 100\n",
            "Translated example sentence 1: \n",
            " ['walkers', 'with', 'the', 'painted', 'in', 'burgundy', 'of', '-', 'person', 'is', 'is', 'down', 'calm', 'calm', '.', '<eos>']\n",
            "Epoch_Loss - 0.11515640467405319\n",
            "\n",
            "Epoch - 86 / 100\n",
            "Translated example sentence 1: \n",
            " ['dressed', 'with', 'the', '3', 'in', 'burgundy', '<unk>', ',', ',', 'is', 'climbing', 'down', 'by', '.', '<eos>']\n",
            "Epoch_Loss - 0.08950038999319077\n",
            "\n",
            "Epoch - 87 / 100\n",
            "Translated example sentence 1: \n",
            " ['workmen', 'with', 'the', 'painted', 'in', 'burgundy', 'blue', ',', 'is', 'down', 'down', 'this', 'weather', '.', '<eos>']\n",
            "Epoch_Loss - 0.1067061498761177\n",
            "\n",
            "Epoch - 88 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'the', 'painted', 'in', 'black', 'overalls', 'is', 'coming', 'down', 'his', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.04169271141290665\n",
            "\n",
            "Epoch - 89 / 100\n",
            "Translated example sentence 1: \n",
            " ['protesters', 'with', 'the', 'painted', 'in', 'in', 'blue', ',', 'is', 'flying', 'down', 'his', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.6767227053642273\n",
            "\n",
            "Epoch - 90 / 100\n",
            "Translated example sentence 1: \n",
            " ['reflections', 'with', 'the', 'painted', 'in', 'full', 'of', 'whom', 'is', 'laying', 'down', 'someone', 'underwater', '.', '<eos>']\n",
            "Epoch_Loss - 0.04512165114283562\n",
            "\n",
            "Epoch - 91 / 100\n",
            "Translated example sentence 1: \n",
            " ['with', 'with', 'the', 'focus', 'in', 'full', 'blue', ',', ',', 'is', 'climbing', 'off', 'traffic', '.', '<eos>']\n",
            "Epoch_Loss - 0.8685057163238525\n",
            "\n",
            "Epoch - 92 / 100\n",
            "Translated example sentence 1: \n",
            " ['with', 'with', 'the', 'face', 'on', 'snow', 'blue', ',', 'is', 'is', 'climbing', 'his', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.1333763301372528\n",
            "\n",
            "Epoch - 93 / 100\n",
            "Translated example sentence 1: \n",
            " ['wall', 'with', 'the', '3', 'on', 'snow', ',', \"'s\", ',', 'is', 'covered', 'covered', 'by', '.', '<eos>']\n",
            "Epoch_Loss - 0.07807255536317825\n",
            "\n",
            "Epoch - 94 / 100\n",
            "Translated example sentence 1: \n",
            " ['wall', 'with', 'the', 'sign', 'in', 'snow', 'blue', ',', 'is', 'is', 'down', 'his', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.36687156558036804\n",
            "\n",
            "Epoch - 95 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'painted', 'in', 'full', 'blue', ',', 'is', 'pointing', 'down', 'calm', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.3548010587692261\n",
            "\n",
            "Epoch - 96 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'the', 'the', 'covered', 'in', 'blue', 'shirt', ',', 'is', 'down', 'his', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.2540363073348999\n",
            "\n",
            "Epoch - 97 / 100\n",
            "Translated example sentence 1: \n",
            " ['golfer', 'with', 'earphones', 'in', 'the', 'down', ',', 'blue', 'he', 'is', 'also', 'climbing', 'above', '.', '<eos>']\n",
            "Epoch_Loss - 0.15331491827964783\n",
            "\n",
            "Epoch - 98 / 100\n",
            "Translated example sentence 1: \n",
            " ['dressed', 'with', 'the', '3', 'on', 'snow', ',', 'whom', 'is', 'pointing', 'by', 'its', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.20776300132274628\n",
            "\n",
            "Epoch - 99 / 100\n",
            "Translated example sentence 1: \n",
            " ['arms', 'with', 'the', '3', 'in', 'down', 'blue', 'and', 'tie', 'is', 'hanging', 'and', 'trunk', '.', '<eos>']\n",
            "Epoch_Loss - 0.1680084466934204\n",
            "\n",
            "Epoch - 100 / 100\n",
            "Translated example sentence 1: \n",
            " ['painted', 'with', 'the', 'painted', 'in', 'in', 'blue', ',', 'whom', 'is', 'climbing', 'by', 'rocks', '.', '<eos>']\n",
            "Epoch_Loss - 0.7941429615020752\n",
            "\n",
            "36.02473482170477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smx7S3beSJRP",
        "outputId": "b352284d-8f70-41ad-abd2-a6ad10b9f573"
      },
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-reddit_tifu').to(device)\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-reddit_tifu')\n",
        "\n",
        "input_text = ' '.join(input_text.split())\n",
        "input_text = 'we went to a vineyard for the holiday weekend . there were pigs there . it was my first time seeing such pigs . our friend wore a funny hat that caught everyone's attention . he was the subject for many jokes . later , we all gathered at the long table to eat and drink wine . '\n",
        "batch = pegasus_tokenizer(input_text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
        "\n",
        "summary_ids = pegasus_model.generate(**batch,\n",
        "                                    num_beams=6,\n",
        "                                    num_return_sequences=1,\n",
        "                                    no_repeat_ngram_size = 2,\n",
        "                                    length_penalty = 1,\n",
        "                                    min_length = 30,\n",
        "                                    max_length = 128,\n",
        "                                    early_stopping = True)\n",
        "\n",
        "output = [pegasus_tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)]\n",
        "output"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['went to a vineyard and saw pigs . friend wore a funny hat that caught everyones attention and was the subject of a lot of jokes.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AazDRHiv5_Dm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}