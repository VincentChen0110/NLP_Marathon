{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day24_HardAttention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CohCbmzfBqRB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0-KJJQHB1V-"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    self.w = nn.Linear((enc_hid_dim * 2) , dec_hid_dim , bias = False)\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, mask):\n",
        "    # hidden bz , dec_hid_dim\n",
        "    # encoder_outputs src len, bz , enc_hid_dim x 2\n",
        "    # mask bz , src len\n",
        "    \n",
        "    batch_size = encoder_outputs.shape[1]\n",
        "    src_len = encoder_outputs.shape[0]\n",
        "\n",
        "    encoder_outputs = self.w(encoder_outputs)\n",
        "    # encoder_outputs src lrn , bz , dec_hid_dim --> converted size\n",
        "\n",
        "    hidden = hidden.unsqueeze(1) \n",
        "    # hidden unsqueeze bz , 1 , dec_hid_dim\n",
        "\n",
        "    attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0)   )\n",
        "    # attention bz, 1 , src len\n",
        "  \n",
        "    attention = attention.squeeze(1)\n",
        "    # squeeze bz , src len\n",
        "\n",
        "    attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    return F.softmax(attention, dim = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77anNRvaCE9H"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        # 這邊開始，同學可以跟隨我們的建議 完成程式 或是自行寫作\n",
        "        # 整理代表 q and k 的 hidden and encoder_output \n",
        "        hidden = self.w(encoder_outputs)hidden.unsqueeze(1) \n",
        "  \n",
        "        encoder_outputs = self.w(encoder_outputs)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        # 計算 向量拼接方式 ATTENTION\n",
        "        # 先將 q and k concat 起來\n",
        "        # 然後經過一層 W attention 自學參數,\n",
        "        # 和一個 tanh activation function.\n",
        "        # 最後乘以一個 Vt 調整成一個同等於input seq 的數列.\n",
        "        attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0)   )\n",
        "        \n",
        "        #attention = [batch size, src len, dec hid dim]\n",
        "        # 將 ATTENTION 結果乘以 V\n",
        "        attention = attention.squeeze(1)\n",
        "        \n",
        "        #attention = [batch size, src len]\n",
        "        # apply MASK 建議使用 tensor 的 masked_fill \n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}