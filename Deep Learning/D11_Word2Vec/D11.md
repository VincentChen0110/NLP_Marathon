### Q1: Why don't we use one-hot encoding, but need other embeddings?
**ANSWER: ** 
1. If the vocabulary is sparse, it will be inefficient
2. If the word embeddings are orthogonal, it cannot fully represent the correlation between words.

### Q2: What's the advantage of Word2Vec?
**ANSWER: **
1. It considers the context words
2. Able to fine tune to add new vocabulary