{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day23_Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEblfxC4Bwni",
        "outputId": "80838620-a32c-4386-f010-a09b6030b339"
      },
      "source": [
        "!pip install torchtext==0.8.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.1\n",
            "  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (1.19.5)\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->torchtext==0.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1 torchtext-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzkMrKflB4b1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import csv\n",
        "import spacy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdjEKKFECGfC",
        "outputId": "c8b31886-99fa-47e1-8683-524307cc2438"
      },
      "source": [
        "#chinese font\n",
        "!wget -O taipei_sans_tc_beta.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_&export=download\n",
        "!mv taipei_sans_tc_beta.ttf /usr/local/lib/python3.7/dist-packages/matplotlib//mpl-data/fonts/ttf\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib.pyplot as plt \n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "import matplotlib.ticker as ticker\n",
        "myfont = FontProperties(fname=r'/usr/local/lib/python3.7/dist-packages/matplotlib/mpl-data/fonts/ttf/taipei_sans_tc_beta.ttf')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-22 13:07:31--  https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.188.102, 64.233.188.113, 64.233.188.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.188.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6r293k2jstat484a89ukg3bofie71gdg/1626959250000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_ [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-07-22 13:07:32--  https://doc-0k-9o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6r293k2jstat484a89ukg3bofie71gdg/1626959250000/02847987870453524430/*/1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\n",
            "Resolving doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)... 74.125.204.132, 2404:6800:4008:c04::84\n",
            "Connecting to doc-0k-9o-docs.googleusercontent.com (doc-0k-9o-docs.googleusercontent.com)|74.125.204.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-font-ttf]\n",
            "Saving to: ‘taipei_sans_tc_beta.ttf’\n",
            "\n",
            "taipei_sans_tc_beta     [  <=>               ]  19.70M  67.9MB/s    in 0.3s    \n",
            "\n",
            "2021-07-22 13:07:33 (67.9 MB/s) - ‘taipei_sans_tc_beta.ttf’ saved [20659344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omx8mO-VCN2s",
        "outputId": "7cafa1ec-8007-4c61-f7b3-b243340c5329"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/NLP_Marathon_DL/Day23_Attention/'\n",
        "lines = open(data_dir + 'cmn.txt' , encoding='utf-8').read().strip().split('\\n')\n",
        "trnslt_pairs = [[s for s in l.split('\\t')] for l in lines ]\n",
        "print (\"Sample: \" , trnslt_pairs[1000][0:2] )\n",
        "print (\"Total records:\" , len(trnslt_pairs))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample:  ['We understand.', '我们明白。']\n",
            "Total records: 26388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfC9wSKNDl7R",
        "outputId": "559e415d-2887-48b8-e9c1-22325881f5aa"
      },
      "source": [
        "trnslt_pairs = [pair for pair in trnslt_pairs if pair[1][0] in ['我','你','他','她']]\n",
        "print (\"Total records after filtering :\" , len(trnslt_pairs))\n",
        "train, test = train_test_split(trnslt_pairs, test_size=0.08)\n",
        "train, val = train_test_split(train, test_size=0.09)\n",
        "print (\"training data:{} , develop data: {} , testing data: {}\".format(len(train),len(val),len(test)))\n",
        "    \n",
        "def write_csv(trn_data, file_path ):\n",
        "    with open(file_path ,'w', newline='', encoding='utf-8') as fout:\n",
        "        writer = csv.writer (fout)\n",
        "        for itm in trn_data: \n",
        "            writer.writerow ([itm[0],itm[1]])\n",
        "            \n",
        "file_path = data_dir + 'train.csv'\n",
        "write_csv(train, file_path )\n",
        "\n",
        "file_path = data_dir + 'val.csv'\n",
        "write_csv(val, file_path )\n",
        "    \n",
        "file_path = data_dir + 'test.csv'\n",
        "write_csv(test, file_path )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total records after filtering : 14416\n",
            "training data:12068 , develop data: 1194 , testing data: 1154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKN_QFxMD_K2",
        "outputId": "1ac73a6c-6cf4-49cd-c220-0550c3a0a9e0"
      },
      "source": [
        "spacy_eng = spacy.load('en_core_web_sm')\n",
        "def tokenize_eng(text):\n",
        "  text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
        "  return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "TRG = Field(tokenize = tokenize_eng, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "def tokenize_cmn(text):\n",
        "  regex = re.compile(r'[^\\u4e00-\\u9fa5A-Za-z0-9]')\n",
        "  text = regex.sub(' ', text)\n",
        "\n",
        "  return [word for word in text if word.strip()]\n",
        "    \n",
        "\n",
        "SRC = Field(tokenize = tokenize_cmn, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            include_lengths = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCXlVJXsEJP_",
        "outputId": "6d01cd42-d619-499c-8317-399e42122818"
      },
      "source": [
        "train_dataset, dev_dataset, test_dataset = TabularDataset.splits(\n",
        "    path = data_dir , format = 'csv', skip_header = True,\n",
        "    train='train.csv', validation='val.csv', test='test.csv',\n",
        "    fields=[\n",
        "        ('trg', TRG),\n",
        "        ('src', SRC)\n",
        "    ]\n",
        ")\n",
        "SRC.build_vocab(train_dataset, min_freq = 1)\n",
        "TRG.build_vocab(train_dataset, min_freq = 1)\n",
        "\n",
        "print (\"中文語料的字元表長度: \" , len(SRC.vocab) , \", 英文的字元表長度: \" ,len(TRG.vocab))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "中文語料的字元表長度:  2758 , 英文的字元表長度:  4231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ngEMCsiEOYi",
        "outputId": "fbfed846-6197-4252-fab8-bea252c42e17"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_dataset, dev_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.src),\n",
        "     device = device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpU7XVNQETFy"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden bz , dec_hid_dim\n",
        "        # encoder_outputs src len, bz , enc_hid_dim x 2\n",
        "        # mask bz , src len\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        hidden = hidden.unsqueeze(1) \n",
        "        # hidden unsqueeze bz , 1 , dec_hid_dim\n",
        "\n",
        "        attention = torch.matmul( hidden , encoder_outputs.permute(1, 2, 0)   )\n",
        "        # attention bz, 1 , src len\n",
        "        \n",
        "        attention = attention.squeeze(1)\n",
        "        # squeeze bz , src len\n",
        "\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Pha11qEovI"
      },
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src shape [src len, batch size]\n",
        "        #src_len shape [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded shape [src len, batch size, emb dim]\n",
        "                \n",
        "        # 使用pack_padded_sequence 來壓縮序列        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        # 使用 pad_packed_sequence 用來展開序列成原本形狀的      \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "            \n",
        "        #outputs shape [src len, batch size, hid dim * num directions]\n",
        "        #hidden shape [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden 堆疊 [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs 是最後一層 \n",
        "        \n",
        "        #hidden [-2, :, : ] 是最後一層 forwards RNN \n",
        "        #hidden [-1, :, : ] 是最後一層 backwards RNN\n",
        "        \n",
        "        # hidden 是最後再過一層 dense layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAYO5VQREsom"
      },
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input shape [batch size]\n",
        "        #hidden shape [batch size, dec hid dim]\n",
        "        #encoder_outputs shape [src len, batch size, enc hid dim * 2]\n",
        "        #mask shape [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input shape [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded shape [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a shape [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a shape [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs shape [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted shape [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted shape [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input shape [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output shape [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden shape [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output shape [1, batch size, dec hid dim]\n",
        "        #hidden shape [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction shape [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSJVRL4tFMPu"
      },
      "source": [
        "class Seq2SeqATTN(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhLOmOR-FMif",
        "outputId": "deae8c0a-34b4-421d-8bb1-1854b9e07194"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 256 # 注意 encoder hidden layer 設定 必須為 dec 的一半 \n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "LEARNING_RATE = 0.002\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = RNNEncoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = RNNDecoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2SeqATTN(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "\n",
        "def initial_mdl_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(initial_mdl_weights)\n",
        "print (\"模型全部參數量: {:10,d} \".format(sum(p.numel() for p in model.parameters())))\n",
        "model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "模型全部參數量: 10,230,407 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqATTN(\n",
              "  (encoder): RNNEncoder(\n",
              "    (embedding): Embedding(2758, 256)\n",
              "    (rnn): GRU(256, 256, bidirectional=True)\n",
              "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): RNNDecoder(\n",
              "    (attention): Attention()\n",
              "    (embedding): Embedding(4231, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=4231, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5d_T03SFUjo",
        "outputId": "544b1c16-ce64-430c-bebb-86e2c4d76c1e"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len.cpu() , trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src, src_len = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, src_len.cpu(), trg, 0) #turn off teacher forcing\n",
        "            \n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "MAX_EPOCHS = 20\n",
        "CLIP = 1\n",
        "model_dir =  '/content/drive/My Drive/Colab Notebooks/NLP_Marathon_DL/Day23_Attention/model'\n",
        "best_valid_loss = 9999999\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    torch.save(model.state_dict(), model_dir + 'model-{}.pt'.format(epoch))\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_dir + 'best-model.pt')\n",
        "   \n",
        "    print (\"Epoch {} training time: {:.2f} sec Training Loss: {:.3f} , Valiation Loss: {:.3f}\".format( epoch , end_time - start_time , train_loss , valid_loss))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 training time: 5.39 sec Training Loss: 5.304 , Valiation Loss: 5.084\n",
            "Epoch 1 training time: 5.42 sec Training Loss: 4.482 , Valiation Loss: 4.877\n",
            "Epoch 2 training time: 5.65 sec Training Loss: 4.008 , Valiation Loss: 4.536\n",
            "Epoch 3 training time: 5.48 sec Training Loss: 3.568 , Valiation Loss: 4.220\n",
            "Epoch 4 training time: 5.45 sec Training Loss: 3.032 , Valiation Loss: 4.029\n",
            "Epoch 5 training time: 5.41 sec Training Loss: 2.545 , Valiation Loss: 3.714\n",
            "Epoch 6 training time: 5.38 sec Training Loss: 2.141 , Valiation Loss: 3.668\n",
            "Epoch 7 training time: 5.32 sec Training Loss: 1.864 , Valiation Loss: 3.563\n",
            "Epoch 8 training time: 5.37 sec Training Loss: 1.559 , Valiation Loss: 3.595\n",
            "Epoch 9 training time: 5.52 sec Training Loss: 1.377 , Valiation Loss: 3.638\n",
            "Epoch 10 training time: 5.32 sec Training Loss: 1.236 , Valiation Loss: 3.647\n",
            "Epoch 11 training time: 5.36 sec Training Loss: 1.119 , Valiation Loss: 3.653\n",
            "Epoch 12 training time: 5.43 sec Training Loss: 1.018 , Valiation Loss: 3.706\n",
            "Epoch 13 training time: 5.53 sec Training Loss: 0.939 , Valiation Loss: 3.783\n",
            "Epoch 14 training time: 5.48 sec Training Loss: 0.808 , Valiation Loss: 3.829\n",
            "Epoch 15 training time: 5.53 sec Training Loss: 0.773 , Valiation Loss: 3.850\n",
            "Epoch 16 training time: 5.46 sec Training Loss: 0.704 , Valiation Loss: 3.927\n",
            "Epoch 17 training time: 5.37 sec Training Loss: 0.641 , Valiation Loss: 4.005\n",
            "Epoch 18 training time: 5.51 sec Training Loss: 0.594 , Valiation Loss: 4.063\n",
            "Epoch 19 training time: 5.42 sec Training Loss: 0.562 , Valiation Loss: 4.104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRMjk7dLFhjc"
      },
      "source": [
        "torch.save(SRC.vocab, model_dir + 'SRC_vocab.pt')\n",
        "torch.save(TRG.vocab, model_dir + 'TRG_vocab.pt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3phxI4OHOjx",
        "outputId": "ffef363c-a235-465f-fd95-ce27923e5cf7"
      },
      "source": [
        "model.load_state_dict(torch.load(model_dir + 'best-model.pt'))\n",
        "#model.load_state_dict(torch.load(model_dir + 'model-7.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.694 | Test PPL:  40.210 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2k0RQTtHYhX"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    model.eval()\n",
        "        \n",
        "    #if isinstance(sentence, str):\n",
        "    #    nlp = spacy_en = spacy.load('en_core_web_sm')\n",
        "    #    tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
        "    #else:\n",
        "    #    tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [token.lower() for token in sentence]\n",
        "        \n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    \n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\n",
        "\n",
        "    mask = model.create_mask(src_tensor)\n",
        "        \n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\n",
        "    \n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
        "\n",
        "        attentions[i] = attention\n",
        "            \n",
        "        pred_token = output.argmax(1).item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9AbJfxxHhQc",
        "outputId": "1caee44a-b92b-4679-dcb3-fba6918fc905"
      },
      "source": [
        "example_idx =520\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['他', '建', '議', '她', '戒', '酒']\n",
            "trg = ['he', 'advised', 'her', 'to', 'stop', 'drinking', '.']\n",
            "predicted trg = ['he', 'advised', 'her', 'to', 'stop', 'smoking', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "323JzM40HlVc"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "        \n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-XlfFNOHxqP",
        "outputId": "142c6ea5-5d7a-4da7-e6c3-fd077de5a092"
      },
      "source": [
        "bleu_score = calculate_bleu(test_dataset, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 16.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwDweYl6H2Sm"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    #fontdict = {\"fontproperties\": zhfont}\n",
        "    \n",
        "    #ax.set_xticks(range(max(max_len_tar, len(predicted_seq))))\n",
        "    #ax.set_xlim(-0.5, max_len_tar -1.5)\n",
        "    \n",
        "    #ax.set_yticks(range(len(sentence) + 2))\n",
        "    #ax.set_xticklabels([subword_encoder_zh.decode([i]) for i in predicted_seq \n",
        "    #                    if i < subword_encoder_zh.vocab_size], \n",
        "    #                   fontdict=fontdict, fontsize=18)\n",
        "    \n",
        "    #plt.rcParams[\"font.family\"]=\"sans-serif\"\n",
        "    #plt.rcParams['font.sans-serif']=['STSong'] #用来正常显示中文标签\n",
        "    \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45 , fontproperties=myfont) #, fontdict=fontdict)\n",
        "    ax.set_yticklabels(['']+translation, fontproperties=myfont) # , fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erwWPZhLUTmR",
        "outputId": "cbea786d-de6f-4f8b-f0e8-1fca47a7162b"
      },
      "source": [
        "example_idx = 620\n",
        "\n",
        "src = vars(train_dataset.examples[example_idx])['src']\n",
        "trg = vars(train_dataset.examples[example_idx])['trg']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['你', '害', '怕', '被', '解', '雇', '吗']\n",
            "trg = ['are', 'you', 'afraid', 'of', 'being', 'fired', '?']\n",
            "predicted trg = ['are', 'you', 'afraid', 'of', 'being', 'fired', '?', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "VomD9CkiUWTW",
        "outputId": "e45b5742-5643-4fc4-c3df-1f87f792f293"
      },
      "source": [
        "print (\"\".join(src ))\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "你害怕被解雇吗\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAAIcCAYAAADBmGulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfXzPdf////vON9sYQowxpyWRJkWjD1LOrUk57XAchT6iOvpk4tMHdUg5OQ5FtMNJMYfqoMlJJQ5ckAjvKZOZOW+ac2OG2Wav7x/92u8Qjpy8n3vO3rfr5fK6XLK9934+nr3fa7der/d7vBzHcQQAAAAjvG0PAAAAUJIRWwAAAAYRWwAAAAYRWwAAAAYRWwAAAAYRWwAAAAYRWwAAAAYRWwAAAAYRWwAAoMQqKCiwPQKxdT1+/SX7+fn5ysvLszwNAAC4Xt7e3srKytK2bduszeBrbeXbSGZmps6fP69PP/1UzZo104MPPihfX/7VAQBQnH3zzTe6ePGiVq5cqbS0NL311lu6++67i3wOiuE/KCgo0Mcff6wDBw6ofPnyWrZsmUqXLq2HH37Y9mgAAOAaMjMztXLlSi1cuFCDBg1S+fLlFRUVpXr16lmZh9j6D9LS0rRv3z717t1b/v7+8vHxUY8ePWyPBQAA/gPHcXT33Xfrgw8+UFhYmLKyshQaGipvb28VFBTI27toX0XFa7au4bvvvlNubq6GDx+uWrVqKT09XUePHpW3t3fha7gAAEDxMmPGDE2ZMkXVq1dXWFiY9u3bp1mzZqlcuXKSVOShJRFbVzVjxgyNGzdOwcHB8vPz0/nz57VixQo1adJEISEh8vLysj0iAAD4jU2bNulf//qXXnzxRYWGhurcuXM6ceKEhg4dqgYNGlibi8uIv3Hu3DmlpqYqPj5eoaGh2r59u3bv3q0+ffqodu3achyH2AIAoBj59WdzTk6OCgoKtGbNGmVkZCg5OVk1a9ZU165drc5HbP1GUFCQ8vPzNXr0aHl7e6t06dI6e/as9uzZo7i4OEILAIBi5vjx46pQoYIeeeQRHT58WGlpaerSpYsefPBBrVq1SmXLlrU6H7H1/1m1apUuXbqk0NBQvffee8rIyFD58uUVEBCgL774Qvv27bM9IgAA+I158+Zp/fr18vf3V926dfXUU0+pQoUKchxHX375pVavXq1+/fpZnZHXbEn6xz/+oenTp2vXrl2aMWOGhgwZoipVqujChQuaOXOmpk2bpscee8z2mAAA4N9s2bJFH3/8sd544w117dpVfn5+mjJlio4fP65vv/1WixYt0vvvv69KlSpZndOjY8txHOXm5mrjxo16++23NWTIEH344YcqKCjQlClTFBoaquDgYMXHx+uuu+6yPa4xv7678ty5c5YnKXp79+61PUKR+/Xx9rR31f663zNnzliepOitXr1aP//8s+0xikxBQYGysrJsj2GNJzzHf/1+zs3NVVRUlCpWrKhWrVqpTZs2On/+vLKzsxUdHa2//vWvqlOnjuVpPTy2zp8/L39/f126dEmZmZmFH+/bt6/y8/MLf69WRESExSnN8/Ly0pYtWxQfH6+kpCTb4xSZcePG6aWXXtLq1as9Kjy8vLzkcrn0+eefe9QPpLy8PG3dulUzZszQ6dOnbY9TZN566y1Nnz5d27dvtz1KkdmyZYvS0tKUnJysgwcP2h6nSE2aNEmzZs3S0aNHbY9i1LFjx5SXl6d69eopPT1dq1atkpeXl2rVqiXHcZSRkSFJKlOmjOVJf+Gxr9lauHChsrKy1K9fP0VHR2vEiBGaNm2aatWqpf3792v//v3Kzc2Vn5+f7VGNcBxHhw4dUrVq1eRyuTRx4kS1bt1amZmZysvLK7H7/tW4ceOUl5enkSNHqmzZsh7zxofPP/9cLpdLu3btUt26deXr66tHHnmk2PwHyZTk5GQtWbJEe/fuVeXKlZWRkaGwsLAS/+7iUaNG6eeff9bs2bPVu3dv3X///QoLC5O/v7/t0Yw5deqUDh8+rO+//17p6eny9vbWiy++qIYNG9oezbh33nlHBw4cUPv27bV161a1b9/e9khGzJ49W2vWrFGDBg0UHBysJ598Ul9++aV2796t8PBw7dy5UzVr1rQ95mV8Ro8ePdr2EEWpoKBAn3/+ueLj4/Xyyy+rTJkyatiwoYKCgjRq1Cilp6dr+fLlGjlypCpUqFAi/0PsOI727dunlJQUHT16VFOmTNF//dd/aeDAgdq+fbu8vLxUoUIF22MaM3LkSB08eFBDhw7V0aNHtWzZMmVlZSkkJES+vr4lOjRTUlKUkJCgv/3tb+rVq5c+//xzpaenq3bt2goICLA9njGpqan64osv9OSTT+rixYv67LPPFBERofDwcNujGZOcnKz169fLx8dHXl5eKl26tJo1a6bExETVrFmzxAbXwoUL9eWXXyorK0t///vflZeXp++//14REREqXbq07fGMmTRpko4cOaIpU6aoVKlSSk1N1ZkzZ7Ry5UpVq1ZNwcHBtke8ZRcvXlRSUpIWLFigadOmaf369Tp//rxiY2MVERGhzZs36/z58xoyZIhq1Khhe9zLeFxsrV27VsePH1fLli1VUFCgpKQkxcfHq23btnrmmWdUv359PfHEEyX60uHOnTtVoUIFLVy4UO+9955ef/11rV27Vjk5OapRo4YmT54sf39/+fn5KSwszPa4bjV+/Hjt27dPPXv21NatW7V8+XKdOHFCDzzwgNLT0+Xr66ty5cqVyMg+ceKE5s6dq9GjR6thw4Y6ffq0PvjgA506dUp+fn6qVKmSAgMDbY/pdhkZGVq0aJF69eqlVatWaffu3frDH/6gNWvWqHHjxvLz8ytRj/exY8e0dOlS1atXTz169NDWrVvVunVrlS1bVi+99JLatGmjqlWraseOHapcubLtcd1qzpw5+vTTTxUcHKxx48Zp48aNevfdd9W8eXNt3LhRERERJfIs7t69e5WRkaE+ffroxIkTioyM1J49ezR9+nRdvHhR5cqVU9WqVeXj42N71FvicrlUuXJlBQcHa926ddqxY4fGjx+vkJAQhYWF6bHHHlOzZs10xx132B71Ch51GfGLL77Q4sWL9dhjjxUWcMuWLdW8eXO98847mjZtmvV3LBSFzZs3Kzs7W0OHDlVQUJBatGih6tWra+LEiQoKCtKYMWO0ePFiBQUFqXz58goJCbE9slv8eulw/PjxWrhwoRYtWqSQkBANGDBAd955p1asWKHmzZvLy8urRF1eWrt2rTZs2KCaNWsqMzNTBw8eVGRkpF555RX16tVLsbGxio+P1+LFixUbG6vQ0FDbI7uFy+XS/Pnz5ePjIz8/P6Wmpmrbtm0aM2aMatasqZ9++klBQUHy9vZWdnZ2iXieX7hwQVOmTFFaWprWrFmj8uXL69tvv1W1atW0bt061a1bV6tWrdLHH3+sESNG2B7XrebMmaP33ntP9913nyZNmqRt27Zp+vTpCg8Pl8vlkpeXl44ePaqIiAjl5+fL17dk/PgbNWqU/P39dc8992jbtm06c+aM8vPz9fXXX2vUqFHy9fXVkiVLFBQUpIceesj2uDdt6dKl+uyzz/TCCy9oyZIl8vPz0/Tp0+Xn56fExETl5OTo6aefLraPa/Gcys0KCgrkOI6OHj2ql19+Wffcc4+ioqJUtWpV+fv7a926dQoICCjRl4/+Xa9evTRlyhQlJyfr0qVL2rx5s44dO6Y33nhDI0eOlJ+fnwYMGFBifgBJv5zd2LJli/77v/9bKSkp2rZtm4KCggpfTLl06VLNnz9fhw8fVlxcnPVfgOdO4eHhOnLkiE6fPq3HH39c27Zt06xZs9SnTx9169ZNkrR161YdP35c4eHhat26tZW/O8zdjh07pg0bNmjQoEHKycnR8ePHVbVqVaWlpembb77R119/LZfLpdq1a8vX11cDBw687S+l+vj4qHTp0nryySd16tQphYeHq3Tp0kpJSVHLli3Vpk0b9e/fX3Xq1FHt2rVtj+s2OTk5chxHjRs31gsvvKBt27Zp9OjR6tmzpzZv3qwDBw6oW7duql69uj788EOFhISoW7dut/2ZntTUVB06dEhPP/20QkNDdfjwYR06dEg//vijevbsqXLlyuntt99WbGyspk2bpvDwcFWrVs322Dfk15/fx48fV1xcnO655x41a9ZMq1at0syZM5Wdna21a9dq8uTJxTa0JA+5jOjl5aXs7GytWLFCISEhysjI0Lhx43THHXdo48aNmjdvnoYPH64qVarYHrVIXLhwQVOnTpWPj49q1aqlJk2a6KOPPlJwcLB69Oih6dOnS5Luuecey5O6T2hoqOrWravw8HB98803Sk9PV//+/VWlShWlpaXphx9+0Jw5c7R+/XqtWLFC7dq1KzFnti5cuKDMzEx5e3urQoUKev7553Xw4EEFBwerYcOGiouLU9WqVTVmzBhFRETc9sEh/fLOww0bNujZZ59VZGSkPvjgA1WsWFF+fn4qXbq0Bg4cqPXr1ysiIkJt27bVAw88UCIC28fHR6VKldL48eOVk5OjJk2ayNvbWwcOHFBqaqrWrVunYcOGKT8/X2vXrlXz5s1tj+wWvr6+qlWrlnJzc/Xjjz9q9uzZCgwMVF5eniQpKipK586d06ZNm+RyudS3b9/Cv5T4dla6dGmVKVNGJ06cUH5+vipVqqSAgAB16dJF5cuX18iRI/Xss8+qTp06+vLLL9W1a1eVKlXK9tg35Nef38uXLy/8+b1mzRrVr19f5cuXl+M4evnll4vdC+J/yyNiq6CgQF9//bXmz5+vMmXKKDc3V507d1aLFi1UqlQpxcTEFLsX05nk7++vjIwMnTt3TiEhIapcubIaNmyoPXv26KGHHtLJkye1evVqtWnTplj/n8KNqlSpksLCwnTp0iXde++9qly5sr755hslJydrxIgRyszM1Nq1a/XKK6+UqDcIBAYGau/evYqKitKOHTt04MABDRw4UImJiZo1a5aqV6+u1157TYGBgSXmRdM+Pj5q0KCBwsPDlZubqw8//FDBwcFq1aqVUlJSdPbsWZUpU0YdO3bUPffcU6JeOB0YGKhSpUoVvjD81KlTql+/viSpSpUq6t69u5o0aaK77rpLQUFBlqd1n4CAAEVEROjIkSNq2rSp0tPTtWfPHnXu3Fnt2rXTmjVrCs/gV69e3fa4buHj46OKFSsqMzNTJ06cUFBQkH766Sfl5eXps88+0xNPPKEWLVpo+/btiomJuS3fEPLbn98XL15U27Zt9fTTT6tRo0Zq2rTpbfHaYi/HQ37B0JkzZ7Rp0ya1bdtWkkrMWYubtX//fuXk5OjgwYPKzMxUQECAcnJy1KtXLy1fvly1a9dWrVq1bI9pxLFjx7Rlyxalp6crPj5eS5cu1ZkzZzR+/HiNGjWqRO47IyNDpUqVkuM4mjNnjsLDw9W9e3e9+eabqlGjhp555hkVFBSUiMuHv3X48GElJCSofPnyKlWqlLp06aLExERlZWUpJibmtruscr2OHj1a+Hqed999V97e3vrzn/+sbt26KTo62vZ4xpw4cUL//Oc/tWnTJj377LOaN2+eKlasqJMnT2rEiBEl8vHOzs7Wpk2btH79ep08eVKvvPKKKlasWHgW63Z/Dep/+vl9u+zNI85sSb/8316tWrXk5eVVYn+o3IiyZcuqQoUKqlChgg4fPixvb2+FhYWpZs2aql27dok4xX4tfn5+CgoK0qOPPqpTp05p/vz5+u677zRy5MgSGVrSL5dRAwMDFRQUpDp16mjjxo1KTU3Viy++qCpVqig4OPi2+A/WzSgoKND58+fVqVMnfffdd/r6669Vrlw59evXr0Q/z0NCQnTfffcpOTlZ5cqVU2hoqBITE/Xkk0+WqDN5v1WqVClFRkZq48aNio6O1v3336958+bprbfeKrFXMPz9/XXnnXcqJydH/v7+atq06WVvdLndv7f/08/v22VvHnNmC9eWlZWllStX6tKlS+rSpUuJeM3O7/n3b9i//OUv6tatW+GlFk9w/PhxJSYmqnfv3iXm3YfX48yZM/rqq6/UunVrj3jnsfTL71bbt2+fOnXqpLNnz3rM471z504tWbJEw4YN04ULF0rUJdNrycnJUU5Ozm1xWc3TEFuQ9MtpaEkl5t2H18PTz3B66v5vl8sO7pKZmamVK1cqNjb2tn/33Y24cOGCvvrqK3Xu3LnEvBYRty9iCwBKuNzcXI8MDk/4q8dweyC2AAAADPK8awgAAABFiNgCAAAwqNj+xsqkpCTbIwAAAFy3qKioq3/CKaZcLpcjycqRkJBgbW2bB/v2rIN9e9bBvov+sCklJcXa2p76eLtcrmv+O+EyIgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEHEFgAAgEFFHluO4xT1kgAAANb4uvsO8/Pz9c477+j7779Xdna2Ro0apaSkJPn7+2v16tXq3r272rRpoxEjRujw4cMqX7683n33XYWGhrp7FAAAAOvcfmbL19dXMTExSkxM1NixYzVt2jRJ0ooVK5SQkKAnn3xS48aNU//+/bVo0SLFxsbq448/dvcYAAAAxYKXY+C6XkZGhhYvXqxdu3Zp+/btiomJUVBQkJ577jlJUnR0tMqXLy9JKigoUHR0tIYNG3bZfSQlJSklJcXdo12XyMhI7d+/38raNrFvz8K+PQv7LnpNmjSxsq4k5eTkKDAw0MraLpfLyrqS3ce7fv36ioqKuvonHTf76aefnM6dOzs//PCDc/LkSadVq1bO5MmTnZkzZxbepmnTps7Zs2f/4/24XC5HkpUjISHB2to2D/btWQf79qyDfRf9YVNKSoq1tT318Xa5XNf8d+L2y4i7du1SjRo11KhRI+3ateuqt2nRooXmzJkjScrOztbZs2fdPQYAAECx4PbYatq0qY4cOaLu3btr+/btV33h++uvv66UlBR16dJF/fv3188//+zuMQAAAIoFt78bsXTp0po/f37hnwcMGHDFbcLCwjR16lR3Lw0AAFDs8EtNAQAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADCK2AAAADPK1PQAAoGT7ZOMGa2tX8/G1tn6zh7pYWVeS4uL6609/HGZtfVyOM1sAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGuSW2Bg8erI0bN0qScnNz1b59eyUnJys2NlZdunTRyJEjlZubK0nq27evtm/fLklauHCh3nzzTXeMAAAAUCy5Jba6d++uJUuWSJI2btyohx56SHFxcXr77be1ZMkS+fj4aPbs2e5YCgAA4Lbi6447adGihSZOnKiLFy9q5cqViomJUVpamurVqydJeuKJJzRhwgQNGDDghu43ISHBHePdsMjISGtr28S+PQv79iw2913Nxy0/am6Kv5eXtfXj4vpbWVeSKt15h7X1s891t7KuVHy/v93yDPT29lbbtm21atUqpaamKiQkRPn5+YWf9/f3l7f3/38SzXGc67rfZ555xh3j3bCEhARra9vEvj0L+/YsNvf9ycYNVtaVfgm99Ev5v39DA94bP8PKutIvoTfe0vrfbVpqZV3J7vPc5XJd83Nue4F8t27dNHXqVEVFRalmzZo6c+aM0tLSJEkLFixQixYtJEl33HGH9u/fr/z8fK1du9ZdywMAABRLbout8PBwBQcHq3379vLz89PEiRM1cuRIdejQQRcuXNAf//hHSVLv3r01adIk/elPf1Ljxo3dtTwAAECx5LYL2RkZGSooKFCjRo0kSQ0aNNCnn356xe2aNGmiNWvWuGtZAACAYs0tsTV79mwtWLBAY8eOdcfdAQAAlBhuia1+/fqpX79+7rgrAACAEoXfIA8AAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGCQr+0BAAAlW4+Hmllbe+fOnXr47rutrN1z01Ir60pS9rnu+s7i+rgcZ7YAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMIrYAAAAMuqnYchxHcXFx6ty5sw4ePOjumQAAAEqMm4qtXbt2KS0tTYsXL1ZERMR1fc20adO0Zs2aKz6+YMECTZky5WbGAAAAKPZ8b+aLsrKyVKlSJXl7X95qjuPIy8vrql8zaNCgm1kKAADgtva7Z7by8/M1ZswYdevWTY8//rg2bNigkSNHyuVy6YUXXtChQ4f0/PPPa/jw4Ro0aJDOnj2rYcOGKTY2Vh07dlRqaqok6bXXXtPXX38tSUpKSlLHjh0VGxurZcuWmd0hAACARb8bW76+voqJiVFiYqLGjh2radOm6Y033lCTJk00depUSdL69ev11FNP6YMPPlBwcLD69eunhQsXauDAgfroo48uu7+8vDzFxcVp4sSJWrhwoerVq2dmZwAAAMXAdV1GLFeunD744APt2rVLhw8fvuLz1atXV+PGjSVJ3t7e8vf316RJk7Rz507l5ORcdtsDBw6oYsWKuvvuuyVJ4eHhyszMvOq6CQkJN7QZd4mMjLS2tk3s27Owb89ic987d+60sq4k5eTkWFvf5vOM53kx4/yOn376yencubPzww8/OCdPnnRatWrlfPfdd86AAQMcx3Gc9PR0p2PHjoW337p1q/PUU085u3fvdpKTk50+ffo4juM4w4YNc5YtW3bZxxzHcebOnetMnjz5inVdLpcjycqRkJBgbW2bB/v2rIN9e9Zhc982paSkWFvbUx9vT923y+W65nPhdy8j7tq1SzVq1FCjRo20a9eu37u5tm3bpvvvv1+1a9e+6u3r1Kmj/fv36/Dhw3IcRzt27Pjd+wQAALhd/W5sNW3aVEeOHFH37t21fft2hYaG/sfbt23bVmvXrlWvXr109uzZKz4fGBio1157Tb1791bPnj0VHBx889MDAAAUc7/7mq3SpUtr/vz5hX8eMGCAJOnBBx+UJFWtWlVffPFF4efDw8P11VdfFf75j3/8oyTpnXfeKfxYp06d1KlTp1scHQAAoPjjr+sBAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwyHpsbdmyRZ07d9b48eNtjwIAAOB2vrYHmDNnjl566SW1bt3a9igAAABuZ/3M1pkzZ1SpUiV5e1sfBQAAwO2K9MzWokWL9Pe//13+/v7q3r27AgIC9OOPP+rVV1/V66+/rhYtWhTlOAAAAMYV2emkvXv3atasWVqwYIEWLFighQsXqk6dOmrQoIEmTpxIaAEAgBLJy3EcpygWmjdvnk6cOKGXXnpJkjRz5kxduHBBmzdvVlxcnO69997Lbp+UlKSUlJSiGO0KkZGR2r9/v5W1bWLfnoV9exab+27SpImVdSUpJydHgYGBVtZ2uVxW1pV4nttQv359RUVFXf2TThFJSEhwJk6cWPjnOXPmOO+//77Tp08fJzk5+Yrbu1wuR5KVIyEhwdraNg/27VkH+/asw+a+bUpJSbG2tqc+3p66b5fLdc3nQpFdRmzevLlWrVql8+fPKzc3V0uXLlV0dHRRLQ8AAGBFkb1AvlatWhowYIB69eqlnJwc9e3bV40aNSqq5QEAAKwo0ncjxsTEKCYm5rKPzZ07tyhHAAAAKFL8cisAAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiIwgDPkAAA5uSURBVC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDrju2Dh06pE6dOl3XbRcsWKAFCxbc9FAAAAAlha+JO+3evbuJuwUAALjt3NBlxPPnzysuLk4dOnTQoEGDlJubq/Xr1ys2NladOnXSjBkzJElTpkzRrFmzJEl9+/bVP/7xD/Xo0UPt27fXgQMHJEnbt29X165d1apVKzVs2FB9+/Z1784AAACKgRuOrSFDhuirr75SYGCg5s2bpxkzZmjevHlasmSJXC6Xjh49esXXZWdn69NPP1XXrl2VmJgoSfrrX/+qoUOHavXq1br33nv11ltvuWdHAAAAxcgNXUa84447VK1aNUlSy5YtNWXKFJ07d049evSQ9EuMXS22Hn74YUlS9erVtW7dOklSWFiYsrKylJeXp6ysLHl5eV3xdQkJCTe2GzeJjIy0trZN7NuzsG/PYnPfO3futLKuJOXk5Fhb3+bzjOd5MeNcp/T0dKdjx46Ff54/f74TExPjDB48+IrbTp482Zk5c6bjOI7Tp08fJzk52XEcx1m2bJkzbNgwx3EcZ/Xq1U779u2dTp06OR999NEV9+FyuRxJVo6EhARra9s82LdnHezbsw6b+7YpJSXF2tqe+nh76r5dLtc1nws3dBnx7Nmzys7O1qVLl7Ro0SL1799f27Zt0969eyVJGRkZ131f33zzjSZMmKClS5eqX79+NzIGAADAbeOGLyMOHTpUe/fuVfv27dWhQweVLVtWr776qiSpZs2amjhx4nXdV61atTR48GCVKVNGAQEBatu2rZ577rkb3wEAAEAxdt2xVbVq1cIXt/+7Zs2a6fPPP7/sY0OGDCn857lz5xb+c7t27dSuXTtJ0tKlS7V8+XL5+/tr37596t27N7EFAABKHCO/Z+t6REdHKzY2Vn5+fgoICNCECRNsjQIAAGCMtdgaPHiwBg8ebGt5AACAIsHfjQgAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGAQsQUAAGCQr+0BAAAlW4HjWFvbsbq+l6V1ba9v7/EurjizBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYBCxBQAAYNAtxdayZcvUqVMnzZ49203jAAAAlCy3FFvTp0/XhAkTdODAAf344483dR8HDx5U3759b2UMAACAYsv3Vr74zJkzqlSpkkaPHu2mcQAAAEqWm46t999/X8eOHdMf/vAHHTt2TDNnztS9996rrl27qmXLltqwYYMSExO1ePFiffjhhyooKNCzzz6rmJgYHT9+XK+88opOnz6tyMhId+4HAACgWLnpy4iDBw9WxYoVNWfOHNWtW7fw47t379bdd9+txMRE7du3TytWrFBiYqISExM1f/585ebmauzYsWrXrp2WLl2qJ554wi0bAQAAKI5u6TLi1fj7+6tDhw6SpI0bNyo5OVndunWTJJ09e1ZnzpzRpk2bNG7cOElSeHj4Ne8rISHB3eNdl8jISGtr28S+PQv79iw2952ammplXUnKycmxtn5Cwhwr60q/Pt721rel2H5/O7egVatWzsmTJ50+ffo4ycnJjuM4zn333Vf4+Tlz5jhvv/32FV8XFRXlXLp0yXEcx9m1a5fTp0+fK27jcrkcSVaOhIQEa2vbPNi3Zx3s27MOm/u+VFBg7diRkmJtbcnL2vHL421rfc98nrtcrmv2ktHfsxUdHa3ly5fr+PHjkqSMjAxJUoMGDbRy5UpJuul3MQIAANwOjMZWzZo19eqrr+pPf/qTYmNjNXfuXEnS8OHDNXnyZD3xxBM6dOiQyREAAACsuqXXbK1evVqSCiNKkr7//vvLbtOxY0d17Njxso/Vq1dPX3zxxa0sDQAAcFvgr+sBAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwiNgCAAAwyNf2AACAki0kuIy1tWfMiFeTqAetrJ15LtvKupJ0aP9+a+uXDQ62sm5xxpktAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg4gtAAAAg6zF1qFDh9S/f3917txZL7/8snJzc22NAgAAYIy12AoICNBf/vIXLV26VKdOndLmzZttjQIAAGCMr62FK1SoIEnKzc3V2bNnVbt2bVujAAAAGGP9NVvjx49Xr169dOedd9oeBQAAwO28HMdxbC1+6tQpvfDCC/rkk0+u+FxSUpJSUlIsTCVFRkZq//79Vta2iX17FvbtWWzu29vbx8q6klS9enUdPHjQytqN7mtkZV1Jyr14Uf4BAVbW/n7rVivrSnaf5/Xr11dUVNRVP2c1ti5cuKD09HTVrVv3is8lJSWpSZMmFqaSEhIS9Mwzz1hZ2yb27VnYt2exue+goFAr60rSjBnx6t//eStrZ5w4YmVdSTq0f7+qRkZaWbtscLCVdSW7z3OXy3XN2LJ6GXHHjh365z//aXMEAAAAo6zG1unTp3XgwAGbIwAAABhl7d2IkvToo4/q0UcftTkCAACAUdbfjQgAAFCSEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAGEVsAAAAG+doeAABQsl26lGdtbcdxrK3v7+NjZV1J8rK8Pi7HmS0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDiC0AAACDbji2fvrpJ50+ffqWFk1NTVVubu4t3QcAAMDt4Lpiy3EcrV27Vs8//7zGjh2r/Px8DR06VF27dlXPnj2VkZEhSVq3bp06d+6srl27atKkSXIcR5L01ltvqV27dnrqqaeUlpam3bt3q1evXvrb3/6mw4cPm9sdAACAZb6/d4Ply5dr3rx5euCBBzRq1ChVrlxZkydP1iOPPKIJEybohx9+UHx8vP785z9rzJgxmjt3ripUqKBBgwbpq6++0sMPP6w1a9ZoxYoVyszMVKlSpVS3bl117NhR3377rcaPHy9J+r//+z+VK1fO+IYBAACK0u/GlvTLma2CgoLCM1Xr1q3Tv/71L82YMUOSFBkZqeTkZN13332qVKmSJKlr165au3atOnTooAYNGmjEiBF67rnnLguqgoICXbp0SV5eXlddNyEh4ZY2d7MiIyOtrW0T+/Ys7Nuz2Ny3l5e9lwfXqFFds2bNtLL2/j17rKwrSRcvXrS2vs3vr+L6/f27sfX444/rscce07p16/Tmm2/KcRxlZmZq2rRpqlevXuHtVq9erfz8/MI/+/v7y9vbW15eXpo0aZK2bt2qYcOGaeDAgcrJydHs2bPVvHlzvfbaa6pSpcpV137mmWfcsMUbl5CQYG1tm9i3Z2HfnsXmvv39A62sK0mzZs3Us88+Z2XtzKxbe33zrdi/Z48ia9e2snaDBg2srCvZfZ67XK5rfu66/nfDy8tLjzzyiOLj4/W///u/atGihebOnSvHcZSbm6uTJ0+qcePGSk5O1rFjx+Q4jj777DO1aNFC2dnZ2r59u+6//3717t1bW7ZsUZ06dfTJJ5/of/7nf64ZWgAAACXBDZ/bjYiI0PDhw1VQUKAuXbqod+/eSktLU9myZTV69Gg9//zzat++verWrav27dvr/Pnzio+PV+fOnTV37lz17NlTd911l/z9/U3sBwAAoFi5rtds/VZAQIDGjh17xcejo6MVHR192ccqVqyoqVOn3tx0AAAAtzl+qSkAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBxBYAAIBBXo7jOLaHuJqkpCTbIwAAAFy3qKioq3682MYWAABAScBlRAAAAIOILQAAAIOILQAAAIOILQAAAIOILQAAAIP+HyTN/7sAYudPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCX-ZnX4Y560"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}